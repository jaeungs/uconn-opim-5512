{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **09: Tree-based Models**\n",
        "- Instructor: [Jaeung Sim](https://jaeungs.github.io/) (University of Connecticut)\n",
        "- Course: OPIM 5512 Data Science Using Python\n",
        "- Date: April 3, 2025"
      ],
      "metadata": {
        "id": "GSYmE0DANNuf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Objectives**\n",
        "1. Implement and evaluate tree-based models for regression and classification problems.\n",
        "1. Understand and implement random forest models.\n",
        "1. Understand and implement gradient boosting and XGBoost.\n",
        "\n",
        "**References**\n",
        "* [Diamonds (Kaggle)](https://www.kaggle.com/datasets/shivam2503/diamonds)\n",
        "* An Introduction to Statistical Learning with Applications in R (ISLR) ([free lectures on YouTube](https://www.youtube.com/@datascienceanalytics1826/))\n",
        "* [Decision Trees (scikit-learn)](https://scikit-learn.org/stable/modules/tree.html)\n",
        "* [Random Forest Regression in Python](https://www.geeksforgeeks.org/random-forest-regression-in-python/)\n",
        "* [How to Solve Overfitting in Random Forest in Python Sklearn?](https://www.geeksforgeeks.org/how-to-solve-overfitting-in-random-forest-in-python-sklearn/?ref=rp)\n",
        "* [Bagging vs Boosting in Machine Learning](https://www.geeksforgeeks.org/bagging-vs-boosting-in-machine-learning/)\n",
        "* [Understanding the Bias-Variance Tradeoff](https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229)\n",
        "* [Understanding Out-of-Bag (OOB) Score: Random Forest Algorithm Evaluation](https://www.analyticsvidhya.com/blog/2020/12/out-of-bag-oob-score-in-the-random-forest-algorithm/)\n",
        "* [StatQuest](https://statquest.org/)\n",
        "* [How to Develop a Gradient Boosting Machine Ensemble in Python](https://machinelearningmastery.com/gradient-boosting-machine-ensemble-in-python/)\n",
        "* [Using XGBoost in Python Tutorial](https://www.datacamp.com/tutorial/xgboost-in-python)"
      ],
      "metadata": {
        "id": "zwQEy_kHNgFG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Part 0. Basic Setup**"
      ],
      "metadata": {
        "id": "WBWuxriTwYm5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import basic libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "_W7nZObYwddV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Diamonds Dataset**\n",
        "\n",
        "We will be working with the Diamonds dataset throughout the tutorial. It is built into the Seaborn library, or alternatively, you can also download it from [Kaggle](https://www.kaggle.com/datasets/shivam2503/diamonds). It has a nice combination of numeric and categorical features of almost 54,000 diamonds.\n",
        "\n",
        ">**Variables**\n",
        "* **price:** price in US dollars (\\$326 - \\$18,823)\n",
        "* **carat:** weight of the diamond (0.2 - 5.01)\n",
        "* **cut:** quality of the cut (Fair, Good, Very Good, Premium, Ideal)\n",
        "* **color:** diamond colour, from J (worst) to D (best)\n",
        "* **clarity:** a measurement of how clear the diamond is (I1 (worst), SI2, SI1, VS2, VS1, VVS2, VVS1, IF (best))\n",
        "* **x:** length in mm (0 - 10.74)\n",
        "* **y:** width in mm (0 - 58.9)\n",
        "* **z:** depth in mm (0 - 31.8)\n",
        "* **depth:** total depth percentage = z / mean(x, y) = 2 * z / (x + y) (43--79)\n",
        "* **table:** width of top of diamond relative to widest point (43--95)"
      ],
      "metadata": {
        "id": "hi4tc1y0wsbv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and explore the dataset\n",
        "diamonds = sns.load_dataset(\"diamonds\")\n",
        "diamonds.head()"
      ],
      "metadata": {
        "id": "TEC6fgZwwsy7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Summary Statistics (Numeric Variables)\n",
        "diamonds.describe()"
      ],
      "metadata": {
        "id": "ZdEul7jZyE9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Summary Statistics (Categorical Variables)\n",
        "diamonds.describe(exclude=np.number)"
      ],
      "metadata": {
        "id": "Dauuv8qkykqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Part 1. Decision Tree Models**"
      ],
      "metadata": {
        "id": "s4plLfqENzfQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Terminology for Trees**\n",
        "* In keeping with the *tree* analogy, the regions divided by the model are called *terminal nodes* or *leaf nodes*.\n",
        "* Decision trees are typically drawn *upside down*, in the sense that the leaves are at the bottom of the tree.\n",
        "* The points along the tree where the predictor space is split are referred to as *internal nodes* or *decision nodes*.\n",
        "\n",
        "<div>\n",
        "<img src=\"https://365datascience.com/resources/blog/rr6cuudl59r-decision-trees-image1.png\" width=\"700\"/>\n",
        "</div>"
      ],
      "metadata": {
        "id": "GGySGCNdj49Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When people make a decision to accept a job offer...\n",
        "\n",
        "<div>\n",
        "<img src=\"https://365datascience.com/resources/blog/59utffqewug-decision-trees-image2.png\" width=\"700\"/>\n",
        "</div>"
      ],
      "metadata": {
        "id": "xn53igP7EcXT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pros and Cons**\n",
        "* Tree-based methods are simple and useful for interpretation.\n",
        "* However, they typically are not competitive with the best supervised learning approaches in terms of prediction accuracy.\n",
        "* Therefore, we also discuss *bagging*, *random forests*, and *boosting*. These methods grow multiple trees which are then combined to yield a single consensus prediction.\n",
        "* Combining a large number of trees can often result in dramatic improvements in prediction accuracy, at the expense of some loss interpretation."
      ],
      "metadata": {
        "id": "py0rEWGbg08m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tree-Based vs. Linear Models in Classification**\n",
        "\n",
        "<div>\n",
        "<img src=\"https://s3.amazonaws.com/media-p.slid.es/uploads/757574/images/6761463/8.7.png\" width=\"700\"/>\n",
        "</div>"
      ],
      "metadata": {
        "id": "yyomFOcoEaCw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.1. Regression**"
      ],
      "metadata": {
        "id": "rsgcGWAYnf_l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Overview of the Tree-Building Process**\n",
        "1. We divide the predictor space --- that is, the set of possible values for $X_1, X_2, ..., X_p$ --- into $J$ distinct and non-overlapping regions, $R_1, R_2, ..., R_J$.\n",
        "1. For every observation that falls into the region $R_j$, we make the same prediction, which is simply the mean of the response values for the training observations in $R_j$."
      ],
      "metadata": {
        "id": "VSWFPhnqlCsm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Objective and Approach**\n",
        "* In theory, the regions could have any shape. However, we choose to divide the predictor space into high-dimensional rectangles, or *boxes*, for simplicity and for ease of interpretation of the resulting predictive model.\n",
        "* The goal is to find boxes $R_1, ..., R_J$ that minimize the RSS, given by\n",
        "> $\\Sigma_{j=1}^J \\Sigma_{i \\in R_j} (y_i - \\hat{y}_{R_j})^2$,\n",
        ">\n",
        "> where $\\hat{y}_{R_j}$ is the mean response for the training observations within the *j*th box.\n",
        "* Unfortunately, it is computationally infeasible to consider every possible partition of the feature space into *J* boxes.\n",
        "* For this reason, we take a *top-down*, *greedy* approach that is known as recursive binary splitting.\n",
        "* The approach is *top-down* because it begins at the top of the tree and then successively splits the predictor space; each split is indicated via two new branches further down on the tree.\n",
        "* It is *greedy* because at each step of the tree building process, the *best* split is made at that particular step, rather than looking ahead and picking a split that will lead to a better tree in some future step."
      ],
      "metadata": {
        "id": "8CNzsEz9t-Bl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Details of the Process**\n",
        "* We first select the predictor $X_j$ and the cutpoint $s$ such that splitting the predictor space into the regions $\\{ X|X_j < s \\}$ and $\\{ X|X_j \\ge s \\}$ leads to the greatest possible reduction in RSS.\n",
        "* Next, we repeat the process, looking for the best predictor and best cutpoint in order to split the data further so as to minimize the RSS within each of the resulting regions.\n",
        "* However, this time, instead of splitting the entire predictor space, we split one of the two previously identified regions. We now have three regions.\n",
        "* Again, we look to split one of these three regions further, so as to minimize the RSS. The process continues until a stopping criterion is reached; for instance, we may continue until no region contains more than a certain number of observations."
      ],
      "metadata": {
        "id": "TFxdK8e-uh5s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from sklearn import tree"
      ],
      "metadata": {
        "id": "-o57XkxJt1Pr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Preparation**"
      ],
      "metadata": {
        "id": "ooeS8wDfbSwq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract feature and target arrays\n",
        "X, y = diamonds.drop('price', axis=1), diamonds[['price']]"
      ],
      "metadata": {
        "id": "a-lC5deYbU_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract text features\n",
        "cats = X.select_dtypes(exclude=np.number).columns.tolist()"
      ],
      "metadata": {
        "id": "FhU5XJ50bVbT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to Pandas category\n",
        "for col in cats:\n",
        "   X[col] = X[col].astype('category')"
      ],
      "metadata": {
        "id": "5kyYiHxBbXEm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a copy of the original DataFrame (otherwise, your model doesn't work)\n",
        "X2 = X.copy()\n",
        "\n",
        "# Categorical columns\n",
        "cut_dummies = pd.get_dummies(X2[\"cut\"], prefix=\"cut\")\n",
        "color_dummies = pd.get_dummies(X2[\"color\"], prefix=\"color\")\n",
        "clarity_dummies = pd.get_dummies(X2[\"clarity\"], prefix=\"clarity\")\n",
        "\n",
        "# Concatenate these variables\n",
        "X2 = pd.concat([X2, cut_dummies, color_dummies, clarity_dummies], axis=1)\n",
        "\n",
        "# Drop categorical variables\n",
        "X2 = X2.drop([\"cut\", \"color\", \"clarity\"], axis=1)\n",
        "X2"
      ],
      "metadata": {
        "id": "BrGi00UVbZp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data\n",
        "X2_train, X2_test, y_train, y_test = train_test_split(X2, y, random_state=10)"
      ],
      "metadata": {
        "id": "J4-DnmVWcU-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Implementation**"
      ],
      "metadata": {
        "id": "jUujnFWKbiHg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "# Create regressor object\n",
        "reg1 = DecisionTreeRegressor(max_depth=2)\n",
        "reg2 = DecisionTreeRegressor(max_depth=5)\n",
        "\n",
        "# fit the regressor with x and y data\n",
        "reg1.fit(X2_train, y_train)\n",
        "reg2.fit(X2_train, y_train)"
      ],
      "metadata": {
        "id": "aYaZaIXAw8zc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict\n",
        "y_pred1 = reg1.predict(X2_test)\n",
        "y_pred2 = reg2.predict(X2_test)"
      ],
      "metadata": {
        "id": "Trna83sqb9HT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the results\n",
        "plt.figure()\n",
        "plt.scatter(X2['carat'], y, s=20, edgecolor=\"black\", c=\"darkorange\", label=\"data\")\n",
        "plt.scatter(X2_test['carat'], y_pred1, color=\"cornflowerblue\", label=\"max_depth=2\", linewidth=1)\n",
        "plt.scatter(X2_test['carat'], y_pred2, color=\"yellowgreen\", label=\"max_depth=5\", linewidth=1)\n",
        "plt.xlabel(\"Carat\")\n",
        "plt.ylabel(\"Price\")\n",
        "plt.title(\"Decision Tree Regression\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "v34RiqLZxCsk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div>\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*xwtSpR_zg7j7zusa4IDHNQ.png\" width=\"500\"/>\n",
        "</div>\n",
        "\n",
        "**Bias vs. Variance in Machine Learning**\n",
        "* **Bias** is the difference between the average prediction of our model and the correct value which we are trying to predict. Model with high bias pays very little attention to the training data and oversimplifies the model. It always leads to high error on training and test data.\n",
        "* **Variance** is the variability of model prediction for a given data point or a value which tells us spread of our data. Model with high variance pays a lot of attention to training data and does not generalize on the data which it hasnâ€™t seen before. As a result, such models perform very well on training data but has high error rates on test data.\n",
        "\n",
        "<div>\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*9hPX9pAO3jqLrzt0IE3JzA.png\" width=\"700\"/>\n",
        "</div>"
      ],
      "metadata": {
        "id": "VnAgtWsQh_Tc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Complexity and Trade-Off**\n",
        "* A complex tree may *overfit* the data, leading to poor test set performance.\n",
        "* A smaller tree with fewer splits (that is, fewer regions $R_1, ..., R_J$) might lead to lower variance and better interpretation at the cost of a little bias.\n",
        "* One possible alternative is to grow the tree only so long as the decrease in the RSS due to each split exceeds some (high) threshold.\n",
        "* This strategy will result in a smaller tress, but is too *short-sighted*: a seemingly worthless split early on in the tree might be followed by a very good split --- that is, a split that leads to a large reduction in RSS later on."
      ],
      "metadata": {
        "id": "mFnElcgtxJdj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pruning a Tree**\n",
        "* A better strategy is to grow a very large tree $T_0$, and then *prune* it back in order to obtain a *subtree*.\n",
        "* *Cost complexity pruning* --- also known as *weakest link pruning* --- is used to do this.\n",
        "* We consider a sequence of trees indexed by a nonnegative tuning parameter $\\alpha$. For each value of $\\alpha$ there corresponds a subtree $T \\subset T_0$ such that\n",
        ">$\\Sigma_{m=1}^{|T|} \\Sigma_{x_i \\in R_m} (y_i - \\hat{y}_{R_m})^2 + \\alpha |T|$,\n",
        ">\n",
        ">is as small as possible. Here |T| indicates the number of terminal nodes of the tree $T$, $R_m$ is the rectangle (i.e., the subset of predictor space) corresponding to the *m*th terminal node, and $\\hat{y}_{R_m}$ is the mean of the training observations in $R_m$."
      ],
      "metadata": {
        "id": "pRisCnOByozM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Choosing the Best Subtree**\n",
        "* The tuning parameter $\\alpha$ controls a trade-off between the subtree's complexity and its fit to the training data.\n",
        "* We select an optimal value $\\hat{\\alpha}$ using cross-validation.\n",
        "* We then return to the full data set and obtain the subtree corresponding to $\\hat{\\alpha}$."
      ],
      "metadata": {
        "id": "9l6OIDWH0rfL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.2. Classification**"
      ],
      "metadata": {
        "id": "oO-QEh36uVnj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Classification Trees**\n",
        "* Very similar to a regression tree, except that it is used to predict a qualitative response rather than a quantitative one.\n",
        "* For a classification tree, we predict that each observation belongs to the *most commonly occurring class* of training observations in the region to which it belongs."
      ],
      "metadata": {
        "id": "bZTu79LF2MNs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Details of Classification Trees**\n",
        "* Just as in the regression setting, we use recursive binary splitting to grow a classification tree.\n",
        "* In the classification setting, RSS cannot be used as a criterion for making the binary splits.\n",
        "* A natural alternative to RSS is the *classification error rate*. This is simply the fraction of the training observations in that region that do not belong to the most common class:\n",
        ">$E=1-max_{k}(\\hat{p}_{mk})$,\n",
        ">\n",
        ">where $\\hat{p}_{mk}$ represents the proportion of training observations in the *m*th region that are from the *k*th class.\n",
        "* However, classification error is not sufficiently sensitive for tree-growing, and in practice two other measures are preferable."
      ],
      "metadata": {
        "id": "uqceARRK_R-p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gini Index**\n",
        "* The *Gini index* is defined by\n",
        ">$G=\\Sigma_{k=1}^K \\hat{p}_{mk} (1 - \\hat{p}_{mk})$,\n",
        ">\n",
        ">a measure of total variance across the *K* classes. The Gini index takes on a small value if all of the $\\hat{p}_{mk}$'s are close to zero or one.\n",
        "* For this reason, the Gini index is referred to as a measure of node *purity* --- a small value indicates that a node contains observations from a single class predominantly.\n",
        "* An alternative to the Gini index is *cross-entropy*, given by\n",
        ">$D=-\\Sigma_{k=1}^K \\hat{p}_{mk} log (\\hat{p}_{mk})$.\n",
        "* It turns out that the Gini index and the cross-entropy are numerically very similar."
      ],
      "metadata": {
        "id": "1gTU56aAAdyG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Preparation**"
      ],
      "metadata": {
        "id": "gLsJY1PFhIEG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract feature and target arrays\n",
        "X, y = diamonds.drop(\"cut\", axis=1), diamonds[['cut']]"
      ],
      "metadata": {
        "id": "85MVaOKggxvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# See how 'y' looks\n",
        "y"
      ],
      "metadata": {
        "id": "OHrGhego1JOE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract text features\n",
        "cats = X.select_dtypes(exclude=np.number).columns.tolist()"
      ],
      "metadata": {
        "id": "NRaGgTtsgxbz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to pd.Categorical\n",
        "for col in cats:\n",
        "   X[col] = X[col].astype('category')"
      ],
      "metadata": {
        "id": "MApmV8AEhEcX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a copy of the original DataFrame\n",
        "X2 = X.copy()\n",
        "\n",
        "# Categorical columns\n",
        "color_dummies = pd.get_dummies(X2[\"color\"], prefix=\"color\")\n",
        "clarity_dummies = pd.get_dummies(X2[\"clarity\"], prefix=\"clarity\")\n",
        "\n",
        "# Concatenate these variables\n",
        "X2 = pd.concat([X2, color_dummies, clarity_dummies], axis=1)\n",
        "\n",
        "# Drop categorical variables\n",
        "X2 = X2.drop([\"color\", \"clarity\"], axis=1)\n",
        "X2"
      ],
      "metadata": {
        "id": "rZEyRiC-hEaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data\n",
        "X2_train, X2_test, y_train, y_test = train_test_split(X2, y, random_state=10)"
      ],
      "metadata": {
        "id": "BZR5wW0ohgZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Implementation**"
      ],
      "metadata": {
        "id": "N8GW55fXhR4p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Estimate the model\n",
        "clf = DecisionTreeClassifier(criterion='gini', max_depth=3, random_state=10)\n",
        "clf.fit(X2_train, y_train)"
      ],
      "metadata": {
        "id": "Yh_u9GELhEXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import tree\n",
        "\n",
        "# Visualize the estimated tree\n",
        "tree.plot_tree(clf)"
      ],
      "metadata": {
        "id": "s6VvCR_lic1X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# More Comprehensible\n",
        "fig, axes = plt.subplots(nrows = 1, ncols = 1, dpi=300)\n",
        "tree.plot_tree(clf, filled = True);"
      ],
      "metadata": {
        "id": "fK6o3mGMkNmw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Total impurity vs effective alpha for train set\n",
        "path = clf.cost_complexity_pruning_path(X2_train, y_train)\n",
        "ccp_alphas, impurities = path.ccp_alphas, path.impurities\n",
        "\n",
        "# Plot the Figure\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(ccp_alphas[:-1], impurities[:-1], marker=\"o\", drawstyle=\"steps-post\")\n",
        "ax.set_xlabel(\"effective alpha\")\n",
        "ax.set_ylabel(\"total impurity of leaves\")\n",
        "ax.set_title(\"Total Impurity vs effective alpha for training set\")"
      ],
      "metadata": {
        "id": "N7mHsxN0hEU4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train a tree using the effective alphas\n",
        "clfs = []\n",
        "for ccp_alpha in ccp_alphas:\n",
        "    clf = DecisionTreeClassifier(random_state=10, ccp_alpha=ccp_alpha)\n",
        "    clf.fit(X2_train, y_train)\n",
        "    clfs.append(clf)\n",
        "print(\n",
        "    \"Number of nodes in the last tree is: {} with ccp_alpha: {}\".format(\n",
        "        clfs[-1].tree_.node_count, ccp_alphas[-1]\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "4k040LvThESO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of Nodes vs. Alpha / Depth vs. Alpha\n",
        "clfs = clfs[:-1]\n",
        "ccp_alphas = ccp_alphas[:-1]\n",
        "\n",
        "node_counts = [clf.tree_.node_count for clf in clfs]\n",
        "depth = [clf.tree_.max_depth for clf in clfs]\n",
        "fig, ax = plt.subplots(2, 1)\n",
        "ax[0].plot(ccp_alphas, node_counts, marker=\"o\", drawstyle=\"steps-post\")\n",
        "ax[0].set_xlabel(\"alpha\")\n",
        "ax[0].set_ylabel(\"number of nodes\")\n",
        "ax[0].set_title(\"Number of nodes vs alpha\")\n",
        "ax[1].plot(ccp_alphas, depth, marker=\"o\", drawstyle=\"steps-post\")\n",
        "ax[1].set_xlabel(\"alpha\")\n",
        "ax[1].set_ylabel(\"depth of tree\")\n",
        "ax[1].set_title(\"Depth vs alpha\")\n",
        "fig.tight_layout()"
      ],
      "metadata": {
        "id": "UcUJr5h1iD9G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Accuracy vs. Alpha for Train and Test Sets\n",
        "train_scores = [clf.score(X2_train, y_train) for clf in clfs]\n",
        "test_scores = [clf.score(X2_test, y_test) for clf in clfs]\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.set_xlabel(\"alpha\")\n",
        "ax.set_ylabel(\"accuracy\")\n",
        "ax.set_title(\"Accuracy vs alpha for training and testing sets\")\n",
        "ax.plot(ccp_alphas, train_scores, marker=\"o\", label=\"train\", drawstyle=\"steps-post\")\n",
        "ax.plot(ccp_alphas, test_scores, marker=\"o\", label=\"test\", drawstyle=\"steps-post\")\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HsJ9dN5xhEKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Part 2. Random Forest**"
      ],
      "metadata": {
        "id": "_0IeUa69t0U2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Forest = A Set of Trees**\n",
        "\n",
        "An ensemble learning method that combines the outputs of multiple decision trees to reach a single result\n",
        "\n",
        "<div>\n",
        "<img src=\"https://www.spotfire.com/content/dam/spotfire/images/graphics/inforgraphics/random-forest-diagram.svg\" width=\"700\"/>\n",
        "</div>"
      ],
      "metadata": {
        "id": "pE-5RAG8G04v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bagging**\n",
        "* *Bootstrap aggregation*, or *bagging*, is a general-purpose procedure for reducing the variance of a statistical learning method; we introduce it here because it is particularly useful and frequently used in the context of decision trees.\n",
        "* Recall that given a set of $n$ independent observations $Z_1, ..., Z_n$, each with variance $\\sigma^2$, the variance of the mean $\\bar{Z}$ of the observations is given by $\\sigma^2/n$.\n",
        "* In other words, *averaging a set of observations reduces variance.* Of course, this is not practical because we generally do not have access to multiple training sets.\n",
        "* Instead, we can bootstrap, by taking repeated samples from the (single) training dataset. In this approach, we generate $B$ different bootstrapped training data sets. We then train our method on the *b*th bootstrapped training set in order to get $\\hat{f}^{*b}$, the prediction at a point $x$. We then average all the predictions to obtain\n",
        ">$\\hat{f}_{bag}(x) = \\frac{1}{B} \\Sigma_{b=1}^B \\hat{f}^{*b}(x)$.\n",
        ">\n",
        ">This is called *bagging*."
      ],
      "metadata": {
        "id": "0UXTxTRKiH0I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Out-of-Bag Error Estimation**\n",
        "* It turns out that there is a very straightforward way to estimate the test error of a bagged model.\n",
        "* Recall that the key to bagging is that trees are repeatedly fit to bootstrapped subsets of the observations. One can show that on average, each bagged tree makes use of around two-thirds of the observations\n",
        "* The remaining one-third of the observations not used to fit a given bagged tree are referred to as the *out-of-bag* (OOB) observations.\n",
        "* We can predict the response for the *i*th observation using each of the trees in which that observation was OOB. This will yield around $B/3$ predictions for the *i*th observation, which we average."
      ],
      "metadata": {
        "id": "_vQ3u-TZao40"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Random Forests**\n",
        "* *Random forests* provide an improvement over bagged trees by way of a small tweak that *decorrelates* the trees. This reduces the variance when we average the trees.\n",
        "* As in bagging, we build decision trees on bootstrapped training samples.\n",
        "* But when building these decision trees, each time a split in a tree is considered, *a random selection of m predictors* is chosen as split candidates from the full set of *p* parameters. The split is allowed to use only one of those *m* predictors.\n",
        "* A fresh selection of *m* predictors is taken at each split, and typically, we choose $m \\thickapprox \\sqrt{p}$. That is, the number of predictors considered at each split is approximately equal to the square root of the total number of predictors."
      ],
      "metadata": {
        "id": "JyIU3cMIaWmy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import the regressor\n",
        "from sklearn.ensemble import RandomForestRegressor"
      ],
      "metadata": {
        "id": "N_pSSD0ySnlG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Preparation**"
      ],
      "metadata": {
        "id": "ma0gbe7rSSBO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract feature and target arrays\n",
        "X, y = diamonds.drop('price', axis=1), diamonds[['price']]"
      ],
      "metadata": {
        "id": "z9mIawupt05w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract text features\n",
        "cats = X.select_dtypes(exclude=np.number).columns.tolist()"
      ],
      "metadata": {
        "id": "QsQ6VnB1SWUf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to Pandas category\n",
        "for col in cats:\n",
        "   X[col] = X[col].astype('category')"
      ],
      "metadata": {
        "id": "VDC95nbvSXcj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the variable types\n",
        "X.dtypes"
      ],
      "metadata": {
        "id": "r67BD_j0SaId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Implementation**"
      ],
      "metadata": {
        "id": "h4dfuCqaSpR9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a copy of the original DataFrame\n",
        "X2 = X.copy()\n",
        "\n",
        "# Categorical columns\n",
        "cut_dummies = pd.get_dummies(X2[\"cut\"], prefix=\"cut\")\n",
        "color_dummies = pd.get_dummies(X2[\"color\"], prefix=\"color\")\n",
        "clarity_dummies = pd.get_dummies(X2[\"clarity\"], prefix=\"clarity\")\n",
        "\n",
        "# Concatenate these variables\n",
        "X2 = pd.concat([X2, cut_dummies, color_dummies, clarity_dummies], axis=1)\n",
        "\n",
        "# Drop categorical variables\n",
        "X2 = X2.drop([\"cut\", \"color\", \"clarity\"], axis=1)\n",
        "X2"
      ],
      "metadata": {
        "id": "VmdpgZJXTOIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create regressor object...doesn't work for categorical covariates\n",
        "regressor = RandomForestRegressor(n_estimators = 100, random_state = 0)\n",
        "\n",
        "# fit the regressor with x and y data\n",
        "regressor.fit(X2, y)"
      ],
      "metadata": {
        "id": "HKZPhSFrTgks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Arrange for creating a range of values from min of 'x' to max of 'x' with a difference of 0.01\n",
        "X_grid = np.arange(min(X2['carat']), max(X2['carat']), 0.01)\n",
        "\n",
        "# Reshape for reshaping the data into a len(X_grid)*1 array, i.e. to make a column out of the X_grid value\n",
        "X_grid = X_grid.reshape((len(X_grid), 1))\n",
        "\n",
        "# Scatter plot for original data\n",
        "plt.scatter(X2['carat'], y, s=20, edgecolor=\"black\", c=\"darkorange\", label=\"data\")\n",
        "\n",
        "# Plot predicted data\n",
        "plt.scatter(X2['carat'], regressor.predict(X2), color=\"yellowgreen\", label=\"randomforest\", linewidth=1)\n",
        "plt.title('Random Forest Regression')\n",
        "plt.xlabel('Carat')\n",
        "plt.ylabel('Price')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ArNJIawrTzL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Solving Overfitting Problem**"
      ],
      "metadata": {
        "id": "eDRZDODpWXiP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data\n",
        "X2_train, X2_test, y_train, y_test = train_test_split(X2, y, random_state=10)"
      ],
      "metadata": {
        "id": "8wIjBPNqWZKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "# Train a default RandomForestRegressor with random_state=10\n",
        "model = RandomForestRegressor(random_state=10)\n",
        "model.fit(X2_train, y_train)\n",
        "print('Training MSE: ',\n",
        "      metrics.mean_squared_error(y_train, model.predict(X2_train)))\n",
        "print('Test MSE: ',\n",
        "      metrics.mean_squared_error(y_test, model.predict(X2_test)))"
      ],
      "metadata": {
        "id": "PALW0PBgXR00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "# Train a default RandomForestRegressor\n",
        "model = RandomForestRegressor(n_estimators=50, max_depth=3, random_state=10)\n",
        "  # The number of trees in the forest = 50 (default 100)\n",
        "  # The maximum depth of the tree = 3 (default None)\n",
        "model.fit(X2_train, y_train)\n",
        "print('Training MSE: ',\n",
        "      metrics.mean_squared_error(y_train, model.predict(X2_train)))\n",
        "print('Test MSE: ',\n",
        "      metrics.mean_squared_error(y_test, model.predict(X2_test)))"
      ],
      "metadata": {
        "id": "4DXdrWp-Ya78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "# Train a default RandomForestRegressor\n",
        "model = RandomForestRegressor(n_estimators=80, max_depth=5, random_state=10)\n",
        "  # The number of trees in the forest = 80 (default 100)\n",
        "  # The maximum depth of the tree = 5 (default None)\n",
        "model.fit(X2_train, y_train)\n",
        "print('Training MSE: ',\n",
        "      metrics.mean_squared_error(y_train, model.predict(X2_train)))\n",
        "print('Test MSE: ',\n",
        "      metrics.mean_squared_error(y_test, model.predict(X2_test)))"
      ],
      "metadata": {
        "id": "EcpkUHF7Zj1C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train your RandomForestRegressor\n",
        "model = RandomForestRegressor(n_estimators=, max_depth=, random_state=10) # Put your `n_estimators` and `max_depth` values\n",
        "model.fit(X2_train, y_train)\n",
        "print('Training MSE: ',\n",
        "      metrics.mean_squared_error(y_train, model.predict(X2_train)))\n",
        "print('Test MSE: ',\n",
        "      metrics.mean_squared_error(y_test, model.predict(X2_test)))"
      ],
      "metadata": {
        "id": "pTPiVDOSsHxA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "More options are described here: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html"
      ],
      "metadata": {
        "id": "_IrI7XBGZLFV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Part 3. Gradient Boosting and XGBoost**"
      ],
      "metadata": {
        "id": "IMZMk6YwuGH2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.1. Boosting**"
      ],
      "metadata": {
        "id": "K4L-vQAqtsJh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Background**\n",
        "* Like bagging, boosting is a general approach that can be applied to many statistical learning methods for regression or classification. We only discuss boosting for decision trees.\n",
        "* Recall that bagging involves creating multiple copies of the original training data set using the bootstrap, fitting a separate decision tree to each copy, and then combining all trees to create a single predictive model.\n",
        "* Notably, each tree is built on a bootstrap data set, independent of the other trees.\n",
        "* Boosting works in a similar way, except that the trees are grown *sequentially*: each tree is grown using information from previously grown trees."
      ],
      "metadata": {
        "id": "ekoxqDCatslG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bagging vs. Boosting**\n",
        ">**Bagging:** It is a homogeneous weak learners' model that learns from each other independently in parallel and combines them for determining the model average.\n",
        ">\n",
        ">**Boosting:** It is also a homogeneous weak learners' model but works differently from bagging. In this model, learners learn sequentially and adaptively to improve model predictions of a learning algorithm.\n",
        "\n",
        "<div>\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/0*Fp5cX4lPib18xb1a.jpeg\" width=\"800\"/>\n",
        "</div>\n",
        "\n",
        "No. | Bagging | Boosting\n",
        "--- | --- | ---\n",
        "1. | The simplest way of combining predictions that belong to the same type. | A way of combining predictions that belong to the different types.\n",
        "2. | Aim to decrease variance (overfitting), not bias. | Aim to decrease bias, not variance.\n",
        "3. | Each model receives equal weight. | Models are weighted according to their performance.\n",
        "4. | Each model is built independently. | New models are influenced by the performance of previously built models.\n",
        "5. | Different training data subsets are selected using row sampling with replacement | Every new subset contains the elements that were misclassified\n",
        " | and random sampling methods from the entire training dataset. | by previous models.\n",
        "6. | Classifiers are trained parallelly. | Classifiers are trained sequentially.\n"
      ],
      "metadata": {
        "id": "5wzv0n2N0hIP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Boosting Algorithm for Regression Trees**\n",
        "1. Set $\\hat{f}(x)=0$ and $r_i = y_i$ for all $i$ in the training set.\n",
        "1. For $b = 1, 2, ..., B$, repeat:\n",
        ">2.1. Fit a tree $\\hat{f}^b$ with $d$ splits ($d+1$ terminal nodes) to the training data $(X, r)$.\n",
        ">\n",
        ">2.2. Update $\\hat{f}$ by adding in a shrunken version of the new tree:\n",
        ">\n",
        ">$\\hat{f}(x) \\leftarrow \\hat{f}(x) + \\lambda \\hat{f}^b(x)$.\n",
        ">\n",
        ">2.3. Update the residuals,\n",
        ">\n",
        ">$r_i \\leftarrow r_i - \\lambda \\hat{f}^b(x)$.\n",
        ">\n",
        "1. Output the boosted model,\n",
        ">\n",
        ">$\\hat{f}(x) = \\Sigma_{b=1}^B \\lambda \\hat{f}^b(x)$.\n"
      ],
      "metadata": {
        "id": "TLgnWzHCugiC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Idea behind this Procedure**\n",
        "* Unlike fitting a single large decision tree to the data, which amounts to *fitting the data hard* and potentially overfitting, the boosting approach instead *learns slowly*.\n",
        "* Given the current model, we fit a decision tree to the residuals from the model. We then add this new decision tree into the fitted function in order to update the residuals.\n",
        "* Each of these trees can be rather small, with just a few terminal nodes, determined by the parameter $d$ in the algorithm.\n",
        "* By fitting small trees to the residuals, we slowly improve $\\hat{f}$ in areas where it does not perform well. The shrinkage parameter $\\lambda$ slows the process down even further, allowing more and different shaped trees to attack the residuals."
      ],
      "metadata": {
        "id": "hoXX1lAOxZYw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hyperparameters for Boosting**\n",
        "1. The *number of trees* $B$.\n",
        "> Unlike bagging and random forests, boosting can overfit if $B$ is too large, although this overfitting tends to occur slowly if at all. We use cross-validation to select $B$.\n",
        "1. The *shrinkage parameter* $\\lambda$, a small positive number.\n",
        ">This controls the rate at which boosting learns. Typical values are 0.01 or 0.001, and the right choice can depend on the problem. Very small $\\lambda$ can require using a very large value of $B$ in order to achieve good performance.\n",
        "1. The *number of splits* $d$ in each tree, which controls the complexity of the boosted ensemble.\n",
        ">Often $d=1$ works well, in which case each tree is a *stump*, considering of a single split and resulting in an additive model. More generally, $d$ is the *interaction depth*, and controls the interaction order of the boosted model, since $d$ splits can involve at most $d$ variables."
      ],
      "metadata": {
        "id": "SgNJYbcIynar"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.2. Gradient Boosting**"
      ],
      "metadata": {
        "id": "Q_8R-Z0-uGZJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gradient Boosting Algorithm for Regression**\n",
        "\n",
        "* **Input:** Data $\\{ (x_i, y_i) \\}_{i=1}^n$, and a differentiable **loss function** $L(y_i, F(x))$\n",
        ">$i$: a data index, $n$: the number of observations\n",
        ">\n",
        ">*Loss function:* the function that a model wants to minimize, e.g., mean squared errors (MSE).\n",
        "\n",
        "* **Step 1:** Initialize model with a constant value: $F_0(x) = argmin_\\gamma \\Sigma_{i=1}^n L(y_i, \\gamma)$\n",
        ">$y_i$: an observed value, $\\gamma$: the predicted values\n",
        ">\n",
        ">For MSE, $F_0(x)$ is the average of $y_i$, and it becomes the initial leaf.\n",
        "\n",
        "* **Step 2:** for $m=1$ to $M$:\n",
        ">**(A)** Compute $r_{im}=-[\\frac{\\partial L(y_i, F(x_i))}{\\partial F(x_i)}]_{F(x)=F_{m-1}(x)}$ for $i = 1, ..., n$\n",
        ">* $r_{im}$ is a residual for sample $i$ and tree $m$.\n",
        ">* $[\\frac{\\partial L(y_i, F(x_i))}{\\partial F(x_i)}]$ is the derivative of the loss function or the ***gradient***.\n",
        ">* $- \\frac{d}{d Predicted} \\frac{1}{2} (Observed - Predicted)^2 = (Observed - Predicted)$\n",
        ">* Now we plug $F_{m-1}(x)$ in for $Predicted$.\n",
        ">\n",
        ">**(B)** Fit a regression tree to the $r_{im}$ values and create terminal regions $R_{jm}$, for $j = 1, ..., J_m$\n",
        ">\n",
        ">**(C)** For $j = 1, ..., J_m$, compute $\\gamma_{jm}=argmin_{\\gamma} \\Sigma_{x_i \\in R_{ij}} L(y_i, F_{m-1}(x_i) + \\gamma)$\n",
        ">* The output value for each leaf is the value for gamma that minimizes $\\gamma_{jm}$ like what we did in **Step 1**.\n",
        ">* Previous prediction $F_{m-1}$ is taken into account.\n",
        ">* The summation considers sample in $R_{ij}$ only.\n",
        ">\n",
        ">**(D)** Update $F_m(x) = F_{m-1}(x) + \\nu \\Sigma_{j=1}^{J_m} \\gamma_{jm} I(x \\in R_{jm})$\n",
        ">* Making a new prediction for each sample\n",
        ">* We add up the output values $\\gamma_{jm}$'s for all the leaves $R_{jm}$ where $x$ can be found.\n",
        ">* $\\nu \\in [0, 1]$ is the learning rate that determines the effect each tree has on the final prediction, and this improves accuracy in the long run.\n",
        "\n",
        "* **Step 3:** Output $F_M(x)$"
      ],
      "metadata": {
        "id": "ZYBT5FeKyTCk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**A. Gradient Boosting for Regression**"
      ],
      "metadata": {
        "id": "ZxfEZVEi9OfP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Preparation**"
      ],
      "metadata": {
        "id": "K00tT_iW77AE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract feature and target arrays\n",
        "X, y = diamonds.drop('price', axis=1), diamonds[['price']]"
      ],
      "metadata": {
        "id": "QPRCM3CHyR8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract text features\n",
        "cats = X.select_dtypes(exclude=np.number).columns.tolist()"
      ],
      "metadata": {
        "id": "xq-8_pwj8IFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to pd.Categorical\n",
        "for col in cats:\n",
        "   X[col] = X[col].astype('category')"
      ],
      "metadata": {
        "id": "jukw3_y9-8ZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the variable types\n",
        "X.dtypes"
      ],
      "metadata": {
        "id": "GKghfCIa8JpT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a copy of the original DataFrame\n",
        "X2 = X.copy()\n",
        "\n",
        "# Categorical columns\n",
        "cut_dummies = pd.get_dummies(X2[\"cut\"], prefix=\"cut\")\n",
        "color_dummies = pd.get_dummies(X2[\"color\"], prefix=\"color\")\n",
        "clarity_dummies = pd.get_dummies(X2[\"clarity\"], prefix=\"clarity\")\n",
        "\n",
        "# Concatenate these variables\n",
        "X2 = pd.concat([X2, cut_dummies, color_dummies, clarity_dummies], axis=1)\n",
        "\n",
        "# Drop categorical variables\n",
        "X2 = X2.drop([\"cut\", \"color\", \"clarity\"], axis=1)\n",
        "X2"
      ],
      "metadata": {
        "id": "z08n-cd-sijR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Implementation**"
      ],
      "metadata": {
        "id": "DvJmNrS_8MS9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries and functions\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedKFold\n",
        "from sklearn.ensemble import GradientBoostingRegressor"
      ],
      "metadata": {
        "id": "4Tkpuq8-7-7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the evaluation procedure\n",
        "cv = RepeatedKFold(n_splits=5, n_repeats=10, random_state=10)\n",
        "  # Number of folds = 5\n",
        "  # Number of times cross-validator needs to be repeated = 10"
      ],
      "metadata": {
        "id": "QT9nAYjc9zWP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic Gradinet Boosting\n",
        "model = GradientBoostingRegressor()\n",
        "\n",
        "# Estimate and evaluate the model\n",
        "n_scores = cross_val_score(model, X2, y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1) # The scikit-learn library makes the MAE negative so that it is maximized instead of minimized\n",
        "print('MAE: %.3f (%.3f)' % (mean(n_scores), std(n_scores))) # Larger negative MAE are better and a perfect model has a MAE of 0."
      ],
      "metadata": {
        "id": "Bj2ffvNO960S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**B. Gradient Boosting for Classification**"
      ],
      "metadata": {
        "id": "OrFNh7C39M5X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Preparation**"
      ],
      "metadata": {
        "id": "Wer7vzekLNdl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract feature and target arrays\n",
        "X, y = diamonds.drop(\"cut\", axis=1), diamonds[['cut']]"
      ],
      "metadata": {
        "id": "FFf_UnZeuGoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "\n",
        "# Encode y to numeric\n",
        "y_encoded = OrdinalEncoder().fit_transform(y)"
      ],
      "metadata": {
        "id": "De0euzTGLA1s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract text features\n",
        "cats = X.select_dtypes(exclude=np.number).columns.tolist()"
      ],
      "metadata": {
        "id": "AcYSpco3LB_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to pd.Categorical\n",
        "for col in cats:\n",
        "   X[col] = X[col].astype('category')"
      ],
      "metadata": {
        "id": "_vp1u0XQLDop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a copy of the original DataFrame\n",
        "X2 = X.copy()\n",
        "\n",
        "# Categorical columns\n",
        "color_dummies = pd.get_dummies(X2[\"color\"], prefix=\"color\")\n",
        "clarity_dummies = pd.get_dummies(X2[\"clarity\"], prefix=\"clarity\")\n",
        "\n",
        "# Concatenate these variables\n",
        "X2 = pd.concat([X2, color_dummies, clarity_dummies], axis=1)\n",
        "\n",
        "# Drop categorical variables\n",
        "X2 = X2.drop([\"color\", \"clarity\"], axis=1)\n",
        "X2"
      ],
      "metadata": {
        "id": "cAw00RjoLcOP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Implementation**"
      ],
      "metadata": {
        "id": "f01RBVOtLP0L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries and functions\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.ensemble import GradientBoostingClassifier"
      ],
      "metadata": {
        "id": "Ybb01oQ9LEtK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "model = GradientBoostingClassifier()\n",
        "\n",
        "# Ddefine the evaluation method\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)"
      ],
      "metadata": {
        "id": "ZWtSRZtYLzoD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Estimate and evaluate the model\n",
        "n_scores = cross_val_score(model, X2, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "print('Mean Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
      ],
      "metadata": {
        "id": "Xa8TrPKDMBPN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.3. XGBoost (eXtreme Gradient Boosting)**"
      ],
      "metadata": {
        "id": "HvgCOjgIuiv0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Basic Concepts**\n",
        "* **Similarity Scores** $= \\frac{\\text{Sum of Residuals, Squared}}{\\text{Number of Residuals} + \\lambda}$\n",
        ">*Sum of residuals, squared* is different from *sum of squared residuals*. Positive and negative residuals cancel out each other.\n",
        "* **Regularization Parameter ($\\lambda$)**\n",
        ">It is intended to reduce the prediction's sensitivity to individual observations.\n",
        ">When $\\lambda > 0$, the **similarity scores** and **gain** are smaller (or easier to **prune** leaves).\n",
        "* **Gain** $= \\text{Similarity}_\\text{Left} + \\text{Similarity}_\\text{Right} - \\text{Similarity}_\\text{Root}$\n",
        ">An improvement in similarity scores after splitting the residuals.\n",
        "* **Pruning**\n",
        ">If the **gain** of splitting the residuals into two groups is greater (smaller) than $\\gamma$, retain (remove) this branch.\n",
        "* **Output Value**  $= \\frac{\\text{Sum of Residuals}}{\\text{Number of Residuals} + \\lambda}$\n",
        "> $\\lambda$ reduces the prediction's sensitivity to an individual observation. Note that the output value for a leaf is simply the average of the residuals in the leaf when $\\lambda=0$.\n",
        "* **Learning Rate ($\\eta$)**\n",
        ">XGBoost makes new predictions by stating with the initial prediction just like other Gradient Boost. And it adds the output of the tree scaled by a **learning rate**. The default value is 0.3."
      ],
      "metadata": {
        "id": "AJZHL_bejzVG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**XGBoost vs Gradient Boosting**\n",
        "* XGBoost is a more regularized form of Gradient Boosting. XGBoost uses advanced regularization (L1 & L2), which improves model generalization capabilities.\n",
        "* XGBoost delivers high performance as compared to basic Gradient Boosting. Its training is very fast and can be parallelized across clusters.\n",
        "\n",
        "**When to use XGBoost?**\n",
        "* When there is a larger number of training samples. Ideally, greater than 1000 training samples and less 100 features or we can say when the number of features < number of training samples.\n",
        "* When there is a mixture of categorical and numeric features or just numeric features.\n",
        "\n",
        "**When not to use XGBoost?**\n",
        "* Image Recognition\n",
        "* Computer Vision\n",
        "* When the number of training samples is significantly smaller than the number of features."
      ],
      "metadata": {
        "id": "7N6nUKaJjz99"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import XGBoost\n",
        "import xgboost as xgb # Already installed in Colab!"
      ],
      "metadata": {
        "id": "hsn-1QCivlQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Preparation**"
      ],
      "metadata": {
        "id": "ZjaivUnM3jTE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract feature and target arrays\n",
        "X, y = diamonds.drop('price', axis=1), diamonds[['price']]"
      ],
      "metadata": {
        "id": "soP3fUk8vwyT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract text features\n",
        "cats = X.select_dtypes(exclude=np.number).columns.tolist()"
      ],
      "metadata": {
        "id": "leZuPO8pz6xm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to Pandas category\n",
        "for col in cats:\n",
        "   X[col] = X[col].astype('category')"
      ],
      "metadata": {
        "id": "ZJTDjOLR0Bhu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the variable types\n",
        "X.dtypes"
      ],
      "metadata": {
        "id": "2djglcZN3stT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a copy of the original DataFrame\n",
        "X2 = X.copy()\n",
        "\n",
        "# Categorical columns\n",
        "color_dummies = pd.get_dummies(X2[\"color\"], prefix=\"color\")\n",
        "clarity_dummies = pd.get_dummies(X2[\"clarity\"], prefix=\"clarity\")\n",
        "\n",
        "# Concatenate these variables\n",
        "X2 = pd.concat([X2, color_dummies, clarity_dummies], axis=1)\n",
        "\n",
        "# Drop categorical variables\n",
        "X2 = X2.drop([\"cut\", \"color\", \"clarity\"], axis=1)\n",
        "X2"
      ],
      "metadata": {
        "id": "-1RPODR16USc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the variable types\n",
        "X2.dtypes"
      ],
      "metadata": {
        "id": "WSeNntLU82cb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data\n",
        "X2_train, X2_test, y_train, y_test = train_test_split(X2, y, random_state=1)"
      ],
      "metadata": {
        "id": "ZDpSMyjm30pq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Implementation**"
      ],
      "metadata": {
        "id": "PuAWMXIO4BVj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate an XGBoost regressor\n",
        "# Objective is reg:squarederror for regression tasks\n",
        "reg = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, seed=42)"
      ],
      "metadata": {
        "id": "Hhg8UVcA7fXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the regressor\n",
        "reg.fit(X2_train, y_train)"
      ],
      "metadata": {
        "id": "WH0hA3Mj8Dyu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# Predict values\n",
        "preds = reg.predict(X2_test)\n",
        "\n",
        "# Define and print MSE\n",
        "mae = mean_absolute_error(y_test, preds)\n",
        "print(f\"MAE of the base model: {mae:.3f}\")"
      ],
      "metadata": {
        "id": "DpU0Cmrk7rQg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}