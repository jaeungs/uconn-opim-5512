{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **#07: Deep Learning Advanced: Hyperparameters**\n",
        "- Instructor: [Jaeung Sim](https://jaeungs.github.io/) (University of Connecticut)\n",
        "- Course: OPIM 5512 Data Science Using Python\n",
        "- Last updated: March 12, 2025"
      ],
      "metadata": {
        "id": "GSYmE0DANNuf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Objectives**\n",
        "1. Implement `tensorflow` to build and train a basic deep learning model.\n",
        "1. Revise hyperparameters to improve model performances.\n",
        "\n",
        "**References**\n",
        "* [Deep Learning Basics by Google Colab](https://colab.research.google.com/github/lexfridman/mit-deep-learning/blob/master/tutorial_deep_learning_basics/deep_learning_basics.ipynb)\n",
        "* [TensorFlow - Single Layer Perceptron by Tutorials Point](https://www.tutorialspoint.com/tensorflow/tensorflow_single_layer_perceptron.htm)\n",
        "* [TensorFlow - Multi-Layer Perceptron Learning by Tutorials Point](https://www.tutorialspoint.com/tensorflow/tensorflow_multi_layer_perceptron_learning.htm)\n",
        "* [TensorFlow - Keras by Tutorials Point](https://www.tutorialspoint.com/tensorflow/tensorflow_keras.htm)"
      ],
      "metadata": {
        "id": "zwQEy_kHNgFG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Part 0. Setup and Data Exploration**"
      ],
      "metadata": {
        "id": "YMCpJs4HOKZM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TensorFlow and tf.keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense\n",
        "\n",
        "# Commonly used modules\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Images, plots, display, and visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import cv2\n",
        "import IPython\n",
        "from six.moves import urllib\n",
        "\n",
        "print(tf.__version__)"
      ],
      "metadata": {
        "id": "LpB0fCqjOcuo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are going to use the Boston housing dataset, which has 506 rows of data, with 13 features in each. Our task is to build a regression model that takes these 13 features as input and output a single value prediction of the \"median value of owner-occupied homes (in $1000).\"\n",
        "\n",
        "Now, we load the dataset. Loading the dataset returns four NumPy arrays:\n",
        "* The `train_features` and `train_labels` arrays are the *training set*â€”the data the model uses to learn.\n",
        "* The model is tested against the *test set*, the `test_features`, and `test_labels` arrays."
      ],
      "metadata": {
        "id": "5qBzA1qnOkh0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "(train_features, train_labels), (test_features, test_labels) = keras.datasets.boston_housing.load_data()"
      ],
      "metadata": {
        "id": "rrIbSiOIOkzL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Explore the shapes of the datasets\n",
        "print(\"Shape of Train Features: \", train_features.shape)\n",
        "print(\"Shape of Train Labels: \", train_labels.shape)\n",
        "print(\"Shape of Test Features: \", test_features.shape)\n",
        "print(\"Shape of Test Label: \", test_labels.shape)"
      ],
      "metadata": {
        "id": "lsksy7EYPuzT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get per-feature statistics (mean, standard deviation) from the training set to normalize by\n",
        "train_mean = np.mean(train_features, axis=0)\n",
        "train_std = np.std(train_features, axis=0)\n",
        "train_features = (train_features - train_mean) / train_std"
      ],
      "metadata": {
        "id": "MhA9J83QPbZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Part 1. Single Layer Perceptron**"
      ],
      "metadata": {
        "id": "WBWuxriTwYm5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Single layer perceptron is the first proposed neural model created. The content of the local memory of the neuron consists of a vector of weights. The computation of a single layer perceptron is performed over the calculation of sum of the input vector each with the value multiplied by corresponding element of vector of the weights. The value which is displayed in the output will be the input of an activation function.\n",
        "\n",
        "![image](https://www.tutorialspoint.com/tensorflow/images/single_layer_perceptron.jpg)"
      ],
      "metadata": {
        "id": "6YRG-UrtLdyx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let us consider the following basic steps of model training:\n",
        "* The weights are initialized with random values at the beginning of the training.\n",
        "* For each element of the training set, the error is calculated with the difference between desired output and the actual output. The error calculated is used to adjust the weights.\n",
        "* The process is repeated until the error made on the entire training set is not less than the specified threshold, until the maximum number of iterations is reached."
      ],
      "metadata": {
        "id": "ML-f7EudMyYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**A. Build and Train the Model**"
      ],
      "metadata": {
        "id": "xcB-OtIcSI1h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building the neural network requires configuring the layers of the model, then compiling the model. First, we stack a few layers together using `keras.Sequential`. Next, we configure the loss function, optimizer, and metrics to monitor. These are added during the model's compile step:\n",
        "\n",
        "* *Loss function* - measures how accurate the model is during training, we want to minimize this with the optimizer.\n",
        "* *Optimizer* - how the model is updated based on the data it sees and its loss function.\n",
        "* *Metrics* - used to monitor the training and testing steps.\n",
        "\n",
        "Here, we will train a single layer perceptron, which does not contain any hidden layer as follows:"
      ],
      "metadata": {
        "id": "7IjsD_xJS2Ju"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model architecture\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(1, input_dim=13, activation='relu') # Single layer, 13 input dimensions, sigmoid activation function\n",
        "])"
      ],
      "metadata": {
        "id": "XLoevYfuTrnC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model with mean absolute error as the loss function and the Adam optimizer\n",
        "model.compile(loss='mae', optimizer='adam', metrics=['mae','mse'])"
      ],
      "metadata": {
        "id": "7JL5iDhyStST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model with a batch size of 200 and 100 epochs\n",
        "model.fit(train_features, train_labels, batch_size=200, epochs=100)"
      ],
      "metadata": {
        "id": "8eAvBxIQSt8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**B. Model Assessment**"
      ],
      "metadata": {
        "id": "jKsPDg3QW8pM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardize test features\n",
        "test_features_std = (test_features - train_mean) / train_std"
      ],
      "metadata": {
        "id": "zaLxKnM2Y3kz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model with standardized features\n",
        "loss = model.evaluate(test_features_std, test_labels)\n",
        "print('Loss:', loss)"
      ],
      "metadata": {
        "id": "ZKPXj8eUXB7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To save the performance\n",
        "performance = {}\n",
        "performance['Single'] = loss"
      ],
      "metadata": {
        "id": "J92JvghyaNzD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Part 2. Multi-Layer Perceptron Learning**"
      ],
      "metadata": {
        "id": "1vZbY6Me0hzS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multi-Layer perceptron defines the most complicated architecture of artificial neural networks. It is substantially formed from multiple layers of perceptron.\n",
        "\n",
        "The diagrammatic representation of multi-layer perceptron learning is as shown below:\n",
        "\n",
        "![image](https://www.tutorialspoint.com/tensorflow/images/multi_layer_perceptron.jpg)"
      ],
      "metadata": {
        "id": "vjVok97NbLSO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**A. Build and Train the Model with 1 Hidden Layer**"
      ],
      "metadata": {
        "id": "XwKkInKlbwyU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's build a network with 1 hidden layer of 20 neurons, and use mean squared error (MSE) as the loss function (most common one for regression problems):"
      ],
      "metadata": {
        "id": "7OfSP1vpUB3L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model architecture\n",
        "model = keras.Sequential([\n",
        "        Dense(20, activation=tf.nn.relu, input_shape=[len(train_features[0])]),\n",
        "        Dense(1)\n",
        "])"
      ],
      "metadata": {
        "id": "A-RelWV_Z7VN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model with mean absolute error as the loss function and the Adam optimizer\n",
        "model.compile(loss='mae', optimizer='adam', metrics=['mae','mse'])"
      ],
      "metadata": {
        "id": "RbkQ7CPzRUv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model with a batch size of 200 and 100 epochs\n",
        "model.fit(train_features, train_labels, batch_size=200, epochs=100)"
      ],
      "metadata": {
        "id": "o4cSUhk4a0yA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**B. Model Assessment**"
      ],
      "metadata": {
        "id": "O7TCKxcVa8Xv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardize test features\n",
        "test_features_std = (test_features - train_mean) / train_std"
      ],
      "metadata": {
        "id": "5cc-57QNa9qA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model with standardized features\n",
        "loss = model.evaluate(test_features_std, test_labels)\n",
        "print('Loss:', loss)"
      ],
      "metadata": {
        "id": "iD1vOO-xa-nE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To save the performance\n",
        "performance['Multi 1'] = loss"
      ],
      "metadata": {
        "id": "WWIx5miubANx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**C. Larger Number of Hidden Layers**"
      ],
      "metadata": {
        "id": "LvD6hu6ncICw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(1) Repeat the process for 2 hidden layers"
      ],
      "metadata": {
        "id": "CYoK71FScWlB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model architecture\n",
        "model = keras.Sequential([\n",
        "        Dense(20, activation=tf.nn.relu, input_shape=[len(train_features[0])]),\n",
        "        Dense(20, activation=tf.nn.relu, input_shape=[len(train_features[0])]), # One more hidden layer\n",
        "        Dense(1)\n",
        "])\n",
        "\n",
        "# Compile the model with mean absolute error as the loss function and the Adam optimizer\n",
        "model.compile(loss='mae', optimizer='adam', metrics=['mae','mse'])\n",
        "\n",
        "# Train the model with a batch size of 200 and 100 epochs\n",
        "model.fit(train_features, train_labels, batch_size=200, epochs=100)"
      ],
      "metadata": {
        "id": "DStMa7O1cIYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model with standardized features\n",
        "loss = model.evaluate(test_features_std, test_labels)\n",
        "print('Loss:', loss)\n",
        "\n",
        "# To save the performance\n",
        "performance['Multi 2'] = loss"
      ],
      "metadata": {
        "id": "g26uFsjJcnVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(2) Repeat the process for 3 hidden layers"
      ],
      "metadata": {
        "id": "RU2qeJnbdCY7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model architecture\n",
        "model = keras.Sequential([\n",
        "        Dense(20, activation=tf.nn.relu, input_shape=[len(train_features[0])]),\n",
        "        Dense(20, activation=tf.nn.relu, input_shape=[len(train_features[0])]),\n",
        "        Dense(20, activation=tf.nn.relu, input_shape=[len(train_features[0])]), # Two more hidden layers\n",
        "        Dense(1)\n",
        "])\n",
        "\n",
        "# Compile the model with mean absolute error as the loss function and the Adam optimizer\n",
        "model.compile(loss='mae', optimizer='adam', metrics=['mae','mse'])\n",
        "\n",
        "# Train the model with a batch size of 200 and 100 epochs\n",
        "model.fit(train_features, train_labels, batch_size=200, epochs=100)"
      ],
      "metadata": {
        "id": "NcIBJI57dYOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model with standardized features\n",
        "loss = model.evaluate(test_features_std, test_labels)\n",
        "print('Loss:', loss)\n",
        "\n",
        "# To save the performance\n",
        "performance['Multi 3'] = loss"
      ],
      "metadata": {
        "id": "B56yVQB1dYhv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**D. Model Comparison**"
      ],
      "metadata": {
        "id": "F11rnGcZdpQv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Saved results (loss / mae / mse)\n",
        "performance"
      ],
      "metadata": {
        "id": "GamEWUK6eoKU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a list of labels for the x-axis\n",
        "labels = ['Loss', 'MAE', 'MSE']\n",
        "\n",
        "# Create a figure and axis object\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "# Set the bar width\n",
        "bar_width = 0.2\n",
        "\n",
        "# Create a list of indices for each bar\n",
        "index = np.arange(len(labels))\n",
        "\n",
        "# Create the bars\n",
        "single = ax.bar(index, performance['Single'], bar_width, label='Single')\n",
        "multi1 = ax.bar(index+bar_width, performance['Multi 1'], bar_width, label='Multi 1')\n",
        "multi2 = ax.bar(index+2*bar_width, performance['Multi 2'], bar_width, label='Multi 2')\n",
        "multi3 = ax.bar(index+3*bar_width, performance['Multi 3'], bar_width, label='Multi 3')\n",
        "\n",
        "# Set the x-axis labels and title\n",
        "ax.set_xlabel('Metrics')\n",
        "ax.set_ylabel('Performance')\n",
        "ax.set_title('Performance by Model')\n",
        "\n",
        "# Set the tick labels and position for the x-axis\n",
        "ax.set_xticks(index + 1.5*bar_width)\n",
        "ax.set_xticklabels(labels)\n",
        "\n",
        "# Add a legend\n",
        "ax.legend()\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BWa-LvjzdvjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Part 3. Experimenting with Hyperparameters**"
      ],
      "metadata": {
        "id": "S36Xl_vFgdzv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your decisions on hyperparameters include:\n",
        "* Number of nodes/neurons\n",
        "* Number of hidden layers\n",
        "* Activation functions\n",
        "* Learning rates\n",
        "* Epochs\n",
        "* Batch sizes\n",
        "* Dropout"
      ],
      "metadata": {
        "id": "zBx7w8YYgpqW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start with the model with 3 hidden layers, which performed the best among the candidates."
      ],
      "metadata": {
        "id": "aaSKwf37hT1h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model architecture\n",
        "model = keras.Sequential([\n",
        "        Dense(20, activation=tf.nn.relu, input_shape=[len(train_features[0])]),\n",
        "        Dense(20, activation=tf.nn.relu, input_shape=[len(train_features[0])]),\n",
        "        Dense(20, activation=tf.nn.relu, input_shape=[len(train_features[0])]), # Two more hidden layers\n",
        "        Dense(1)\n",
        "])\n",
        "\n",
        "# Compile the model with mean absolute error as the loss function and the Adam optimizer\n",
        "model.compile(loss='mae', optimizer='adam', metrics=['mae','mse'])\n",
        "\n",
        "# Train the model with a batch size of 200 and 100 epochs\n",
        "model.fit(train_features, train_labels, batch_size=200, epochs=100)"
      ],
      "metadata": {
        "id": "bq79Wa0qhIxJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model with standardized features\n",
        "loss = model.evaluate(test_features_std, test_labels)\n",
        "print('Loss:', loss)\n",
        "\n",
        "# To save the performance\n",
        "new_performance = {}\n",
        "new_performance['Baseline'] = loss"
      ],
      "metadata": {
        "id": "okIASwkkhNVQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see from the above codes, you can easily revise number of nodes, hidden layers, activation functions, epochs, and batch sizes. Thus, here we focus on learning rates and dropout."
      ],
      "metadata": {
        "id": "gZe61iCbhbLM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**A. Learning Rates**"
      ],
      "metadata": {
        "id": "cHVPO9oqiQWu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The default learning rate for most optimizers in Keras (such as the Adam optimizer) is 0.001.\n",
        "\n",
        "Let's try 1) a smaller rate (0.0005), and 2) a larger rate (0.005)."
      ],
      "metadata": {
        "id": "4RiN3tTCigLd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model architecture\n",
        "model = keras.Sequential([\n",
        "        Dense(20, activation=tf.nn.relu, input_shape=[len(train_features[0])]),\n",
        "        Dense(20, activation=tf.nn.relu, input_shape=[len(train_features[0])]),\n",
        "        Dense(20, activation=tf.nn.relu, input_shape=[len(train_features[0])]), # Two more hidden layers\n",
        "        Dense(1)\n",
        "])\n",
        "\n",
        "# Define the optimizer with a learning rate of 0.0005\n",
        "opt = keras.optimizers.Adam(learning_rate=0.0005)\n",
        "\n",
        "# Compile the model with mean absolute error as the loss function and the Adam optimizer\n",
        "model.compile(loss='mae', optimizer=opt, metrics=['mae','mse']) # Use the revised optimizer\n",
        "\n",
        "# Train the model with a batch size of 200 and 100 epochs\n",
        "model.fit(train_features, train_labels, batch_size=200, epochs=100)"
      ],
      "metadata": {
        "id": "w4ljN1YAiVTo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model with standardized features\n",
        "loss = model.evaluate(test_features_std, test_labels)\n",
        "print('Loss:', loss)\n",
        "\n",
        "# To save the performance\n",
        "new_performance['Learning 0.0005'] = loss"
      ],
      "metadata": {
        "id": "m9Vg1OlZiVCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model architecture\n",
        "model = keras.Sequential([\n",
        "        Dense(20, activation=tf.nn.relu, input_shape=[len(train_features[0])]),\n",
        "        Dense(20, activation=tf.nn.relu, input_shape=[len(train_features[0])]),\n",
        "        Dense(20, activation=tf.nn.relu, input_shape=[len(train_features[0])]), # Two more hidden layers\n",
        "        Dense(1)\n",
        "])\n",
        "\n",
        "# Define the optimizer with a learning rate of 0.005\n",
        "opt = keras.optimizers.Adam(learning_rate=0.005)\n",
        "\n",
        "# Compile the model with mean absolute error as the loss function and the Adam optimizer\n",
        "model.compile(loss='mae', optimizer=opt, metrics=['mae','mse']) # Use the revised optimizer\n",
        "\n",
        "# Train the model with a batch size of 200 and 100 epochs\n",
        "model.fit(train_features, train_labels, batch_size=200, epochs=100)"
      ],
      "metadata": {
        "id": "88HL5_rSjFZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model with standardized features\n",
        "loss = model.evaluate(test_features_std, test_labels)\n",
        "print('Loss:', loss)\n",
        "\n",
        "# To save the performance\n",
        "new_performance['Learning 0.005'] = loss"
      ],
      "metadata": {
        "id": "PlFsn-cwjFwL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**B. Dropout**"
      ],
      "metadata": {
        "id": "Q4vtb9enjYgb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model architecture\n",
        "model = keras.Sequential([\n",
        "        Dense(20, activation=tf.nn.relu, input_shape=[len(train_features[0])]),\n",
        "        Dropout(0.2), # Dropout rates of 20%\n",
        "        Dense(20, activation=tf.nn.relu),\n",
        "        Dropout(0.2), # Dropout rates of 20%\n",
        "        Dense(20, activation=tf.nn.relu),\n",
        "        Dropout(0.2), # Dropout rates of 20%\n",
        "        Dense(1)\n",
        "])\n",
        "\n",
        "# Compile the model with mean absolute error as the loss function and the Adam optimizer\n",
        "model.compile(loss='mae', optimizer='adam', metrics=['mae','mse'])\n",
        "\n",
        "# Train the model with a batch size of 200 and 100 epochs\n",
        "model.fit(train_features, train_labels, batch_size=200, epochs=100)"
      ],
      "metadata": {
        "id": "BpRJK-Ewjgk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model with standardized features\n",
        "loss = model.evaluate(test_features_std, test_labels)\n",
        "print('Loss:', loss)\n",
        "\n",
        "# To save the performance\n",
        "new_performance['Dropout 0.2'] = loss"
      ],
      "metadata": {
        "id": "oCNxMvYpjr5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model architecture\n",
        "model = keras.Sequential([\n",
        "        Dense(20, activation=tf.nn.relu, input_shape=[len(train_features[0])]),\n",
        "        Dropout(0.4), # Dropout rates of 40%\n",
        "        Dense(20, activation=tf.nn.relu),\n",
        "        Dropout(0.4), # Dropout rates of 40%\n",
        "        Dense(20, activation=tf.nn.relu),\n",
        "        Dropout(0.4), # Dropout rates of 40%\n",
        "        Dense(1)\n",
        "])\n",
        "\n",
        "# Compile the model with mean absolute error as the loss function and the Adam optimizer\n",
        "model.compile(loss='mae', optimizer='adam', metrics=['mae','mse'])\n",
        "\n",
        "# Train the model with a batch size of 200 and 100 epochs\n",
        "model.fit(train_features, train_labels, batch_size=200, epochs=100)"
      ],
      "metadata": {
        "id": "ZU_IksOmjwwk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model with standardized features\n",
        "loss = model.evaluate(test_features_std, test_labels)\n",
        "print('Loss:', loss)\n",
        "\n",
        "# To save the performance\n",
        "new_performance['Dropout 0.4'] = loss"
      ],
      "metadata": {
        "id": "_BwC4WgDjxF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**C. Model Comparison**"
      ],
      "metadata": {
        "id": "T2JZtWcqj-iq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Saved results (loss / mae / mse)\n",
        "new_performance"
      ],
      "metadata": {
        "id": "KxxnfT54kOVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a list of labels for the x-axis\n",
        "labels = ['Loss', 'MAE', 'MSE']\n",
        "\n",
        "# Create a figure and axis object\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "# Set the bar width\n",
        "bar_width = 0.15\n",
        "\n",
        "# Create a list of indices for each bar\n",
        "index = np.arange(len(labels))\n",
        "\n",
        "# Create the bars\n",
        "ax1 = ax.bar(index, new_performance['Baseline'], bar_width, label='Baseline')\n",
        "ax2 = ax.bar(index+bar_width, new_performance['Learning 0.0005'], bar_width, label='Learning 0.0005')\n",
        "ax3 = ax.bar(index+2*bar_width, new_performance['Learning 0.005'], bar_width, label='Learning 0.005')\n",
        "ax4 = ax.bar(index+3*bar_width, new_performance['Dropout 0.2'], bar_width, label='Dropout 0.2')\n",
        "ax4 = ax.bar(index+4*bar_width, new_performance['Dropout 0.4'], bar_width, label='Dropout 0.4')\n",
        "\n",
        "# Set the x-axis labels and title\n",
        "ax.set_xlabel('Metrics')\n",
        "ax.set_ylabel('Performance')\n",
        "ax.set_title('Performance by Model')\n",
        "\n",
        "# Set the tick labels and position for the x-axis\n",
        "ax.set_xticks(index + 1.5*bar_width)\n",
        "ax.set_xticklabels(labels)\n",
        "\n",
        "# Add a legend\n",
        "ax.legend()\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bR2T7MJ-j9iP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}