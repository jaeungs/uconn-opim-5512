{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **10: Reinforcement Learning (Q-Learning)**\n",
        "- Instructor: [Jaeung Sim](https://jaeungs.github.io/) (University of Connecticut)\n",
        "- Course: OPIM 5512 Data Science Using Python\n",
        "- Date: April 17, 2025"
      ],
      "metadata": {
        "id": "wZ2W_xFgFu69"
      },
      "id": "wZ2W_xFgFu69"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important notes:** This notebook is fully based on [Tsuyoshi Matsuzaki's reinforcement learning tutorials (GitHub)](https://github.com/tsmatz/reinforcement-learning-tutorials). All credits should be given to [Tsuyoshi Matsuzaki](https://tsmatz.wordpress.com/about/), and the instructor does not argue his originality at all."
      ],
      "metadata": {
        "id": "uf-PJYiVGDEl"
      },
      "id": "uf-PJYiVGDEl"
    },
    {
      "cell_type": "markdown",
      "id": "pleasant-witch",
      "metadata": {
        "id": "pleasant-witch"
      },
      "source": [
        "## **Q-Learning in Reinforcement Learning**\n",
        "\n",
        "Q-Learning is most primitive, but big part of algorithms to learn reinforcement learning.\n",
        "\n",
        "In order to understand how it works, first let's consider the expected rewards as follows.\n",
        "\n",
        "$$ R = \\sum_{t=0}^{\\infty} {\\gamma^t r_t} $$\n",
        "\n",
        "where $r_t$ is a reward value obtained at $t$ and $\\gamma$ is discount.\n",
        "\n",
        "For instance, when you try to grab an object, you will do the following 3 actions :\n",
        "\n",
        "- Action #1 : Stretch your arm ($t=0$)<br>\n",
        "  Getting reward 0.\n",
        "- Action #2 : Open your hand ($t=1$)<br>\n",
        "  Getting reward 0.\n",
        "- Action #3 : Grab an object ($t=2$)<br>\n",
        "  Getting reward 10.\n",
        "\n",
        "In this case, you will get a reward value 10 on action #3 ($t=2$), however the action #1 ($t=0$) is obviously contributing to the final rewards. Hence, we consider that the action #1 will have the following expected cumulative reward.<br>\n",
        "Here we assume $\\gamma$ is 0.99.\n",
        "\n",
        "$$ R_{t=0} = 0 + 0.99 \\times 0 + 0.99^2 \\times 10 = 9.801 $$\n",
        "\n",
        "Same as above, $R_{t=1} = 9.9, R_{t=2} = 10$.\n",
        "\n",
        "Q-value is based on this idea of expected cumulative reward. Depending on each state (observation), the each action will have the corresponding expected reward.<br>\n",
        "In above example, if you see an object in front of you (i.e, the **state** of \"you see an object\"), the **action** \"stretching your arm\" will have high value of expected reward. However, if you cannot see an object anywhere, the action \"stretching your arm\" will have low value of expected reward.\n",
        "\n",
        "Q-value of each corresponding state and action is denoted as $Q(s, a)$. Suppose both action and state has 1 dimension of discrete values, $Q(s, a)$ will be written as a table (called Q-Table) as follows.<br>\n",
        "If the state is s2, the optimal action to pick up will be action a2. If s3, the optimal action will be action a4.\n",
        "\n",
        "![Q-Table](https://raw.githubusercontent.com/tsmatz/reinforcement-learning-tutorials/d34677967c7fd62e0aa4541e316ab5244400ce3d/assets/q-table.png)\n",
        "\n",
        "In practice, both action space and observation space may have more than 1 dimension. For instance, in CartPole example (below example), the returned state (observation) has 4 elements of float values, i.e, 4 dimensions. (See [readme.md](https://github.com/tsmatz/reinforcement-learning-tutorials/) for CartPole.) Then Q-Table will be the combination of 1 dimension (action space) and 4 dimension (observation space).\n",
        "\n",
        "In Q-Learning, we optimize this table by the following iterative updates ($t=0,1,2,\\ldots$).<br>\n",
        "In the following equation, $ Q_t(s_t,a_t) $ is current Q-value and $ Q_{t+1}(s_t,a_t) $ is the updated Q-value.\n",
        "\n",
        "$$ Q_{t+1}(s_t,a_t) = Q_t(s_t,a_t) + \\alpha \\left( r_t + \\gamma \\max_a{Q_t(s_{t+1},a)} - Q_t(s_t,a_t) \\right) $$\n",
        "\n",
        "where $\\alpha$ is learning rate.\n",
        "\n",
        "This equation means that :\n",
        "\n",
        "- Suppose, you executed an action $a_t$ on state $s_t$, and as a result, you got reward $r_t$ and the state has changed to $s_{t+1}$.\n",
        "- The optimal next action will satisfy $a_{t+1}=\\max_{a}{Q(s_{t+1},a)}$.<br>\n",
        "  By taking this optimal action, you will then get the expected reward : $r_t + \\gamma \\max_{a}{Q(s_{t+1},a)}$.\n",
        "- Compare this optimal q-value with current q-value $Q(s_t,a_t)$ in q-table. Then update this current value $Q(s_t,a_t)$ by learning rate $\\alpha$.<br>\n",
        "  This will result into above equation.\n",
        "\n",
        "Now let's build this Python example with CartPole environment. (See [readme.md](https://github.com/tsmatz/reinforcement-learning-tutorials/) about CartPole.)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "terminal-minute",
      "metadata": {
        "id": "terminal-minute"
      },
      "source": [
        "First, please install the required packages and import these modules."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "promotional-portfolio",
      "metadata": {
        "id": "promotional-portfolio"
      },
      "outputs": [],
      "source": [
        "# Instructor's note: You can skip this in the Colab environment\n",
        "!pip install numpy gymnasium matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "creative-chess",
      "metadata": {
        "id": "creative-chess"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "together-perspective",
      "metadata": {
        "id": "together-perspective"
      },
      "source": [
        "CartPole has 4 elements of continuous (float) observation space. In order for applying primitive Q-Learning, we should convert continuous state to discrete state (i.e, **discretize**).\n",
        "\n",
        "In this example, we will convert Tuple(Box, Box, Box, Box) into Tuple(Discrete(20), Discrete(20), Discrete(20), Discrete(20)) - which converts float value to the bin of value for each segment.\n",
        "\n",
        "![discretize](https://raw.githubusercontent.com/tsmatz/reinforcement-learning-tutorials/d34677967c7fd62e0aa4541e316ab5244400ce3d/assets/discretize.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Instructor's note:**\n",
        "\n",
        "We create discritization bins for each of the 4 state variables:\n",
        "1. Cart position\n",
        "1. Cart velocity\n",
        "1. Pole angle\n",
        "1. Pole angular velocity"
      ],
      "metadata": {
        "id": "tK1Z6qXC_9u5"
      },
      "id": "tK1Z6qXC_9u5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "honey-house",
      "metadata": {
        "id": "honey-house"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "\n",
        "new_observation_shape = (20, 20, 20, 20)\n",
        "\n",
        "bins = []\n",
        "for i in range(4):\n",
        "    item = np.linspace( # Create 20 evenly spaced values in the specified range\n",
        "        # Min/Max for each dimension (for i=0,2)\n",
        "        env.observation_space.low[i] if (i == 0) or (i == 2) else -4, # For i=1,3, the actual bounds are unbounded,\n",
        "        env.observation_space.high[i] if (i == 0) or (i == 2) else 4, # so it defaults to [-4,4] for stability\n",
        "        num=new_observation_shape[i],\n",
        "        endpoint=False)\n",
        "    item = np.delete(item, 0) # Removes the first edge to match `np.digitize()` expectations\n",
        "    bins.append(item)\n",
        "    print(bins[i]) # Each `bins[i]` is a 1D array of bin edges for dimension i\n",
        "\n",
        "# define function to convert to discrete state\n",
        "def get_discrete_state(s):\n",
        "    new_s = []\n",
        "    for i in range(4):\n",
        "        new_s.append(np.digitize(s[i], bins[i]))\n",
        "    return new_s"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ignored-cincinnati",
      "metadata": {
        "id": "ignored-cincinnati"
      },
      "source": [
        "Now we generate Q-Table $Q(s,a)$ and initialize all values by 0. (Here it's 5 dimensional table.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "needed-communications",
      "metadata": {
        "id": "needed-communications"
      },
      "outputs": [],
      "source": [
        "q_table = np.zeros(new_observation_shape + (env.action_space.n,))\n",
        "q_table.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "julian-sacrifice",
      "metadata": {
        "id": "julian-sacrifice"
      },
      "source": [
        "Now, update Q-Table with above Q-Learning algorithm.\n",
        "\n",
        "However, in the beginning, Q-Table was initialized all by zeros (not optimized at all) and will always pick up wrong actions. Therefore, the action is randomly picked up to explore in the first stage, and when it grows to learn, it then picks up the optimal actions with Q-Table gradually using the following coefficient parameter $\\epsilon$ to control. (This exploration algorithm is called **Epsilon-Greedy**.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fitted-torture",
      "metadata": {
        "id": "fitted-torture"
      },
      "outputs": [],
      "source": [
        "gamma = 0.99\n",
        "alpha = 0.1\n",
        "epsilon = 1\n",
        "epsilon_decay = epsilon / 4000\n",
        "\n",
        "# pick up action from q-table with greedy exploration\n",
        "def pick_sample(s, epsilon):\n",
        "    # get optimal action,\n",
        "    # but with greedy exploration (to prevent picking up same values in the first stage)\n",
        "    if np.random.random() > epsilon:\n",
        "        a = np.argmax(q_table[tuple(s)])\n",
        "    else:\n",
        "        a = np.random.randint(0, env.action_space.n)\n",
        "    return a\n",
        "\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "reward_records = []\n",
        "for i in range(6000):\n",
        "    # Run episode till done\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "    s, _ = env.reset()\n",
        "    s_dis = get_discrete_state(s)\n",
        "    while not done:\n",
        "        a = pick_sample(s_dis, epsilon)\n",
        "        s, r, term, trunc, _ = env.step(a)\n",
        "        done = term or trunc\n",
        "        s_dis_next = get_discrete_state(s)\n",
        "\n",
        "        # Update Q-Table\n",
        "        maxQ = np.max(q_table[tuple(s_dis_next)])\n",
        "        q_table[tuple(s_dis)][a] += alpha * (r + gamma * maxQ - q_table[tuple(s_dis)][a])\n",
        "\n",
        "        s_dis = s_dis_next\n",
        "        total_reward += r\n",
        "\n",
        "    # Update epsilon for each episode\n",
        "    if epsilon - epsilon_decay >= 0:\n",
        "        epsilon -= epsilon_decay\n",
        "\n",
        "    # Record total rewards in episode (max 500)\n",
        "    print(\"Run episode{} with rewards {}\".format(i, total_reward), end=\"\\r\")\n",
        "    reward_records.append(total_reward)\n",
        "\n",
        "print(\"\\nDone\")\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12ac75ce",
      "metadata": {
        "id": "12ac75ce"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# Generate recent 50 interval average\n",
        "average_reward = []\n",
        "for idx in range(len(reward_records)):\n",
        "    avg_list = np.empty(shape=(1,), dtype=int)\n",
        "    if idx < 50:\n",
        "        avg_list = reward_records[:idx+1]\n",
        "    else:\n",
        "        avg_list = reward_records[idx-49:idx+1]\n",
        "    average_reward.append(np.average(avg_list))\n",
        "# Plot\n",
        "plt.plot(reward_records)\n",
        "plt.plot(average_reward)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lyric-butterfly",
      "metadata": {
        "id": "lyric-butterfly"
      },
      "source": [
        "As you can see above, this method won't work in large continuous and stochastic spaces (e.g, continuous action space), since this method will need so many discrete mesh for solving problems.\n",
        "\n",
        "If you're interested in such problems, please refer to policy gradient methods in the [Tsuyoshi Matsuzaki's reinforcement learning tutorials (GitHub)](https://github.com/tsmatz/reinforcement-learning-tutorials)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "modified-champagne",
      "metadata": {
        "id": "modified-champagne"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}