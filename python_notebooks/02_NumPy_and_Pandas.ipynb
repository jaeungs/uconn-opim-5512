{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **#02: NumPy and Pandas**\n",
        "- Instructor: [Jaeung Sim](https://jaeungs.github.io/) (University of Connecticut)\n",
        "- Course: OPIM 5512: Data Science Using Python\n",
        "- Last updated: January 23, 2025"
      ],
      "metadata": {
        "id": "JxjzMe8Qj5Jb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Objectives**\n",
        "1. Understand data structure in Python.\n",
        "2. Process a dataset using NumPy and Pandas.\n",
        "\n",
        "**Contents**\n",
        "* Part 1: `NumPy`\n",
        "* Part 2: `Pandas`\n",
        "* Part 3: Data Processing with External Data\n",
        "\n",
        "**References**\n",
        "* [Welcome to Colab!](https://colab.research.google.com/)\n",
        "* [NumPy User Guide](https://numpy.org/doc/stable/user/index.html)\n",
        "* [Pandas Tutorial - W3Schools](https://www.w3schools.com/python/pandas/)\n",
        "* [Pandas User Guide](https://pandas.pydata.org/docs/user_guide/)\n",
        "* [Introduction to Data Science with Python](https://nustat.github.io/DataScience_Intro_python/)"
      ],
      "metadata": {
        "id": "EhMbHph3kCSt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Part 1: `NumPy`**"
      ],
      "metadata": {
        "id": "J_VaelaE28-R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`NumPy` (**Num**erical **Py**thon) provides support for large, multi-dimensional arrays and matrices, along with a collection of mathematical functions to operate on these data structures, and it is highly efficient for numerical calculations."
      ],
      "metadata": {
        "id": "A1tXSQii3Gkw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install NumPy library\n",
        "!pip install numpy # Already installed"
      ],
      "metadata": {
        "id": "tEaDFpVS3CGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import NumPy\n",
        "import numpy as np # Shorten the imported name to np for better readability of code using NumPy"
      ],
      "metadata": {
        "id": "Wf96rmn9_eAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1.1. Array**"
      ],
      "metadata": {
        "id": "-FzTtuSHEYNu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "An **array** is a central data structure of the `NumPy` library. It is a grid of values and it contains information about the raw data, how to locate an element, and how to interpret an element. It has a grid of elements that can be indexed in various ways."
      ],
      "metadata": {
        "id": "Q3XV-WVCEeQK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dimension of arrays**\n",
        "* One-dimensional: like **a list**\n",
        "* Two-dimensional: like **a table**\n",
        "* Three-dimensional: like **a set of tables**\n",
        "* An arbitrary number of dimensions? Generalized as `ndarray`!\n",
        "\n",
        "<img src=\"https://nustat.github.io/DataScience_Intro_python/Datasets/numpy_image.png\" width=\"1000\" height=\"350\">"
      ],
      "metadata": {
        "id": "j8CVEbxhGAQ5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most `NumPy` arrays have some restrictions. For instance:\n",
        "* All elements of the array must be of the **same type** of data.\n",
        "* Once created, the **total size** of the array **can't change**.\n",
        "* The shape must be **rectangular**, **not jagged**\n",
        "  * e.g., each row of a two-dimensional array must have the same number of columns."
      ],
      "metadata": {
        "id": "NkxzLh3TJFYT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.1.1. Creating arrays from scratch**"
      ],
      "metadata": {
        "id": "XyTDuOALE6-G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create and print example arrays\n",
        "a = np.array([1, 2, 3, 4, 5, 6])\n",
        "A = np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]])\n",
        "print(a[0])\n",
        "print(a[1])\n",
        "print(A[0])\n",
        "print(A[1])"
      ],
      "metadata": {
        "id": "MyB7r945EYw8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create various arrays\n",
        "np.zeros(3) # An array filled with 0's"
      ],
      "metadata": {
        "id": "0SgHbkQm6JW5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.ones(3) # An array filled with 1's"
      ],
      "metadata": {
        "id": "ofrh8vAH6Lqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.arange(5) # An array with a range of elements"
      ],
      "metadata": {
        "id": "Z85mM1zC6MyW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.arange(10, 50, 10)) # An array that contains a range of evenly spaced intervals\n",
        "print(np.arange(10, 51, 10)) # Specify the first number, last number, and the step size"
      ],
      "metadata": {
        "id": "GFoFEb2F6PmG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.linspace(0, 10, num=4)) # An array with values that are spaced linearly in a specified interval\n",
        "print(np.linspace(0, 10, num=5))\n",
        "print(np.linspace(0, 10, num=6))"
      ],
      "metadata": {
        "id": "ShNchDCS6VNW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.1.2. Basic atributes of `numpy` array**"
      ],
      "metadata": {
        "id": "wxDBrcrbKJUx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's play with array `a` and `A`\n",
        "a"
      ],
      "metadata": {
        "id": "BfAFs1pj6ZN9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A"
      ],
      "metadata": {
        "id": "g9E2ke6f6qNp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`ndim`: shows the number of dimensions (or axes) of the array"
      ],
      "metadata": {
        "id": "0q6J0a_pKSrg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"a.ndim is\", a.ndim)\n",
        "print(\"A.ndim is\", A.ndim)"
      ],
      "metadata": {
        "id": "pCJ25K-HKNcF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`shape`: the size of the array in each dimension. $(m, n)$ for $m$ rows and $n$ columns. The length of the shape tuple is the rank or the number of dimensions `ndim`."
      ],
      "metadata": {
        "id": "MYkw4vogKXnP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"a.shape is\", a.shape)\n",
        "print(\"A.shape is\", A.shape)"
      ],
      "metadata": {
        "id": "haHEUz1RKX98"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`size`: the total number of elements of the array, equivalent to the product of the elements of shape"
      ],
      "metadata": {
        "id": "-oiozU4oKY4v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"a.size is\", a.size)\n",
        "print(\"A.size is\", A.size)"
      ],
      "metadata": {
        "id": "2uVlDlu-KZJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`dtype`: the type of elements in the array\n",
        "\n",
        "* Data type examples are available here: <https://www.geeksforgeeks.org/python-data-types/>\n",
        "\n"
      ],
      "metadata": {
        "id": "9vFyByERKZg3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"a.dtype is\", a.dtype)\n",
        "print(\"A.dtype is\", A.dtype)"
      ],
      "metadata": {
        "id": "wiRTUnHLKZvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`T`: used to transpose the `NumPy` array"
      ],
      "metadata": {
        "id": "D4k0c5BMKakD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a.T"
      ],
      "metadata": {
        "id": "BU0CD-OXKa34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A.T"
      ],
      "metadata": {
        "id": "SCSNgYwO7foJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.1.3. Creating arrays from existing data**"
      ],
      "metadata": {
        "id": "ZmaTQwWWFA2F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Picking a section of an existing array"
      ],
      "metadata": {
        "id": "fO5zTr5o7yxD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Begin with an existing array\n",
        "a = np.array([1,  2,  3,  4,  5,  6,  7,  8,  9, 10])"
      ],
      "metadata": {
        "id": "9fUvz2J2FBDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new array from a section of your array\n",
        "arr1 = a[3:8]\n",
        "arr1"
      ],
      "metadata": {
        "id": "dLoFhSaR74vC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stacking arrays using `np.vstack()` and `np.hstack()`"
      ],
      "metadata": {
        "id": "lis0eqXk8JnY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Begin with two existing arrays\n",
        "a1 = np.array([[1, 1], [2, 2]])\n",
        "a2 = np.array([[3, 3], [4, 4]])\n",
        "print(a1)\n",
        "print(a2)"
      ],
      "metadata": {
        "id": "Y47aThrb78PT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stack these arrays\n",
        "print(np.vstack((a1, a2))) # Vertically\n",
        "print(np.hstack((a1, a2))) # Horizontally"
      ],
      "metadata": {
        "id": "yvgrWKjw79MN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reshaping arrays with `reshape` and `hsplit`"
      ],
      "metadata": {
        "id": "_E-jvDby9Ar0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Begin with a long array\n",
        "x = np.arange(1, 25)\n",
        "x"
      ],
      "metadata": {
        "id": "_tYSGGhy9CGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshape it as a (2x12) array\n",
        "x = np.reshape(x, (2,12))\n",
        "x"
      ],
      "metadata": {
        "id": "qFHPl9Pe97th"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the reshaped array\n",
        "print(np.hsplit(x, 3)) # Into three equally shaped arrays\n",
        "np.hsplit(x, (3, 4)) # After the third and fourth column"
      ],
      "metadata": {
        "id": "BsioFOIK9-t8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.1.4. Array operations**"
      ],
      "metadata": {
        "id": "VofveWGDFJZV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can add, subtract, multiplicate, and divide arrays. Also, you can use various statistical functions, such as maximum, minimum, sum, mean, product, and standard deviation."
      ],
      "metadata": {
        "id": "CUwWeKWw-jAb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Array example #1"
      ],
      "metadata": {
        "id": "CUADNVaj-zIU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define two arrays\n",
        "data = np.array([1, 2])\n",
        "ones = np.ones(2, dtype=int) # An array with 1's\n",
        "print(data)\n",
        "print(ones)"
      ],
      "metadata": {
        "id": "k4WKX2WVFJq-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add, subtract, multiplicate, and divide arrays\n",
        "print(data + ones)\n",
        "print(data - ones)\n",
        "print(data * data)\n",
        "print(data / data)"
      ],
      "metadata": {
        "id": "OnMQ88Tq-uJT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the maximum, minimum, and sum of the elements in 'data' array\n",
        "print(data.max())\n",
        "print(data.min())\n",
        "print(data.sum())"
      ],
      "metadata": {
        "id": "mPNyVY6D-wdZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the mean, product, and standard deviation of the elements in 'data' array\n",
        "print(data.mean())\n",
        "print(data.prod())\n",
        "print(data.std())"
      ],
      "metadata": {
        "id": "CoMjLw9a-xPO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Array example #2"
      ],
      "metadata": {
        "id": "YnQx0lAY-0zS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define another array\n",
        "a = np.array([[0.45053314, 0.17296777, 0.34376245, 0.5510652],\n",
        "              [0.54627315, 0.05093587, 0.40067661, 0.55645993],\n",
        "              [0.12697628, 0.82485143, 0.26590556, 0.56917101]])"
      ],
      "metadata": {
        "id": "2PwAzN5R-2lP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the maximum, minimum, and sum of the elements in 'a' array\n",
        "print(a.max())\n",
        "print(a.min())\n",
        "print(a.sum())"
      ],
      "metadata": {
        "id": "44hxzGT4-3wy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(a.max(axis=0)) # axis=0 indicates the column level\n",
        "print(a.min(axis=0))\n",
        "print(a.sum(axis=0))"
      ],
      "metadata": {
        "id": "akv9hPWz-4z1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(a.max(axis=1)) # axis=1 indicates the row level\n",
        "print(a.min(axis=1))\n",
        "print(a.sum(axis=1))"
      ],
      "metadata": {
        "id": "NB0Eev2s-53J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise #1.** Create an array (`A`) having each of the following arrays as its rows. The first array (`a1`) has integers from 1 to 5 as elements, and the second array (`a2`) has five elements filled with zeros."
      ],
      "metadata": {
        "id": "A5QDaA9U-933"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a1 = np.arange( , , ) # Create an array [1, 2, 3, 4, 5]\n",
        "a2 = np.zeros() # Create an array [0, 0, 0, 0, 0]\n",
        "A = np.array([, ]) # Create a 2D array with a1 and a2 as rows"
      ],
      "metadata": {
        "id": "KGjzBaql_BYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the code to show the results\n",
        "print(a1)\n",
        "print(a2)\n",
        "print(A)"
      ],
      "metadata": {
        "id": "Qbjr3jQm_IFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Part 2: `Pandas`**"
      ],
      "metadata": {
        "id": "FNIz8YCH208E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Pandas` provides two primary data structures: `Series` (one-dimensional) and `DataFrame` (two-dimensional, like a table). With Pandas, you can easily clean, filter, transform, and visualize data, making it an essential tool for data wrangling in data science and analytics. It is built on top of `NumPy` and integrates seamlessly with other Python libraries."
      ],
      "metadata": {
        "id": "ElmQ4DL798W7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this code to import the NumPy and Pandas modules\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "EMxiq-lEEghC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2.1. Series**\n",
        "\n",
        "Series is a one-dimensional labeled array capable of holding any data type (integers, strings, floating point numbers, Python objects, etc.). The axis labels are collectively referred to as the index."
      ],
      "metadata": {
        "id": "Kh4QuTPl0_Nc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Series (`Pandas`) vs. Array (`NumPy`)**\n",
        ">\n",
        "> While Series (`pandas`) and array (`numpy`) may seem similar as one-dimensional arrays, their differences in terms of data handling capabilities, efficiency, and use cases make them suitable for different scenarios in data analysis and scientific computing.\n",
        ">\n",
        "> **1. Use Case:** If you need labeled data, are working with heterogeneous data, or need to align data by labels, `Series` is more suitable. It's also preferable when working with tabular data and when integrating with `pandas` DataFrames.\n",
        ">\n",
        "> **2. Performance:** For numerical operations, especially on large datasets where performance is a concern and where data homogeneity is maintained, `array` (`numpy`) is generally faster and more memory-efficient.\n",
        ">\n",
        "> **3. Functionality:** `Series` provides more functionalities (like handling missing data seamlessly) that are very useful in data analysis and manipulation, especially in data science workflows.\n",
        ">"
      ],
      "metadata": {
        "id": "iLI539w5Sk0Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.1.1. Creating a Series**"
      ],
      "metadata": {
        "id": "62NMx-4i8Rsp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a series from a list\n",
        "s1 = [1, 7, 2]\n",
        "myvar = pd.Series(s1)\n",
        "print(myvar)"
      ],
      "metadata": {
        "id": "oHlzlcHf2ciA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(myvar[0]) # Call the first item"
      ],
      "metadata": {
        "id": "pIC9cmh73f4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a series with labels\n",
        "s1 = [1, 7, 2]\n",
        "myvar = pd.Series(s1, index=[\"x\", \"y\", \"z\"])\n",
        "print(myvar)"
      ],
      "metadata": {
        "id": "8uUrAM6e29Ym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(myvar[0]) # Call an item with a number\n",
        "print(myvar[\"x\"]) # Call an item with an index"
      ],
      "metadata": {
        "id": "GzkbMtRQ3WCF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a series from dictionary 'd1'\n",
        "d1 = {\"a\": 0.0, \"b\": 1.0, \"c\": 2.0}\n",
        "pd.Series(d1)"
      ],
      "metadata": {
        "id": "u211b6ui4l3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.Series(d1, index=[\"d\", \"c\", \"b\", \"a\"]) # Change the index order"
      ],
      "metadata": {
        "id": "_-AbEBRk5Nkf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a series from a dictionary 'calories'\n",
        "calories = {\"day1\": 1420, \"day2\": 1380, \"day3\": 1390}\n",
        "myvar = pd.Series(calories, index = [\"day1\", \"day2\"]) # Insufficient index to store all items\n",
        "print(myvar) # Only first two days will appear"
      ],
      "metadata": {
        "id": "bhWG-T6436Xd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.1.2. Exploring Series and Operations**"
      ],
      "metadata": {
        "id": "pE3LWZIA8KLE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a Series\n",
        "s2 = pd.Series(np.random.randn(5), index=[\"a\", \"b\", \"c\", \"d\", \"e\"])\n",
        "print(s2)"
      ],
      "metadata": {
        "id": "7HemLACgBSYS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Series is ndarray-like\n",
        "print(s2[0]) # 1st item\n",
        "print(\"----------\")\n",
        "print(s2[:3]) # Before 4th item\n",
        "print(\"----------\")\n",
        "print(s2[s2 > s2.median()]) # Items above median\n",
        "print(\"----------\")\n",
        "print(s2[[1, 2, 3]]) # 2nd - 4th items"
      ],
      "metadata": {
        "id": "QrrhpH0F7qel"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Series is dict-like\n",
        "print(s2[\"a\"]) # Item with index \"a\"\n",
        "print(s2[\"e\"])\n",
        "print(\"e\" in s2) # If \"e\" is in series \"s2\"\n",
        "print(\"f\" in s2)\n",
        "print(s2.get(\"e\")) # Item with index \"e\"\n",
        "print(s2.get(\"f\")) # Return 'None'"
      ],
      "metadata": {
        "id": "cueeICL8LZqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorized operations\n",
        "print(s2 + s2)\n",
        "print(\"----------\")\n",
        "print(s2 - s2)\n",
        "print(\"----------\")\n",
        "print(s2 * s2)\n",
        "print(\"----------\")\n",
        "print(s2 / s2)\n",
        "print(\"----------\")\n",
        "print(np.exp(s2)) # Take exponential to each item\n",
        "print(\"----------\")\n",
        "print(s2[1:] + s2[:-1]) # Integrate two sub-series"
      ],
      "metadata": {
        "id": "D5qSrnDgPMgK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 2. Exploring the sum of three uniform distributions**\n",
        "\n",
        "Suppose you have three variables $x_1$, $x_2$, and $x_3$, which are independently drawn from the identical distribution $U[0,1]$. How would the distribution of $x_1 + x_2 + x_3$ look like?\n",
        "\n",
        "To explore this with simulation, revise the following incomplete code to satisfy the followings:\n",
        "\n",
        "1. Define three series (`s3`, `s4`, `s5`), each of which has 100 random numbers drawn from a uniform distribution between 0 and 1. All three series are independent of each other.\n",
        "\n",
        "   **Hint:** `np.random.uniform(a,b,n)` draws `n` random numbers from a uniform distribution with a range [`a`, `b`).\n",
        "\n",
        "2. Define series `s345` that consists of the sum of `s3`, `s4`, and `s5`.\n",
        "\n"
      ],
      "metadata": {
        "id": "WcskMcpPQc5N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Complete and run the code following the instruction\n",
        "s3 = pd.Series(np.random.uniform(0, 1, 1000)) # (1) Define a series having `1000` random numbers from a `uniform` distribution between `0` and `1`\n",
        "s4 = pd.Series(np.random.uniform(0, 1, 1000)) # (1) Define a series having `1000` random numbers from a `uniform` distribution between `0` and `1`\n",
        "s5 = pd.Series(np.random.uniform(0, 1, 1000)) # (1) Define a series having `1000` random numbers from a `uniform` distribution between `0` and `1`"
      ],
      "metadata": {
        "id": "xIxo4boGQj55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the code to load Matplotlib\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "t5fW_WttVHuQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the code to draw s3's histogram\n",
        "plt.hist(s3)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RUhNakGlY7Y7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the code to draw s4's histogram\n",
        "plt.hist(s4)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Mi16C7ixZELr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the code to draw s5's histogram\n",
        "plt.hist(s5)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gl_Mo0w5cy3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Complete and run the code following the instruction\n",
        "s345 = s3 + s4 + s5 # (2) Define a series with the sum of s3, s4, and s5"
      ],
      "metadata": {
        "id": "mLMFW8OuS6L1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the code to draw s34's histogram\n",
        "plt.hist(s345)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9r9Qh6vRUcFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Takeaway:** As you add more variables following a common uniform distribution, their sum's distribution becomes closer to a normal distribution (for details, refer to [**the Central Limit Theorem**](https://en.wikipedia.org/wiki/Central_limit_theorem))."
      ],
      "metadata": {
        "id": "68mpf8Kzeceu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2.2. DataFrame**\n",
        "\n",
        "DataFrame is a 2-dimensional labeled data structure with columns of potentially different types. You can think of it like a spreadsheet or SQL table, or a dict of Series objects. It is generally the most commonly used `pandas` object.\n",
        "\n"
      ],
      "metadata": {
        "id": "vwGOGKk27O4-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.2.1. Creating and Exploring a DataFrame**"
      ],
      "metadata": {
        "id": "QdRzUc3EQNDL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create data with two arrays\n",
        "d1 = {\n",
        "  \"calories\": [420, 380, 390, 522],\n",
        "  \"duration\": [50, 40, 45, 36]\n",
        "}\n",
        "\n",
        "# Load data into a DataFrame object\n",
        "df1 = pd.DataFrame(d1)\n",
        "\n",
        "print(df1)"
      ],
      "metadata": {
        "id": "P0eSZCTT7pOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Explore rows\n",
        "print(df1.loc[0]) # 1st row\n",
        "print('------------')\n",
        "print(df1.loc[[0, 2]]) # 1st and 3rd rows"
      ],
      "metadata": {
        "id": "QHBjkdpOeWzF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Explore rows with named indexes\n",
        "df1 = pd.DataFrame(d1, index = [\"day1\", \"day2\", \"day3\", \"day4\"])\n",
        "\n",
        "df1"
      ],
      "metadata": {
        "id": "rlw0pagTfYCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df1.loc[\"day1\"]) # 1st row\n",
        "print('------------')\n",
        "print(df1.loc[0]) # Error message"
      ],
      "metadata": {
        "id": "nUsSQ2ZxfwBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Explore columns\n",
        "print(df1[\"calories\"]) # 'calories' column\n",
        "print('------------')\n",
        "print(df1[\"duration\"]) # 'duration' column\n",
        "print('------------')\n",
        "print(df1[[\"calories\", \"duration\"]]) # 'calories' and 'duration' column"
      ],
      "metadata": {
        "id": "W-InAh_Oe0vg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add columns\n",
        "df1[\"joules\"] = df1[\"calories\"] * 4.184\n",
        "df1[\"hours\"] = df1[\"duration\"] / 60\n",
        "df1[\"minutes\"] = \"minutes\" # See what happens\n",
        "df1[\"seconds\"] = df1[\"duration\"][:2] * 60 # Restrict observations to 2nd row\n",
        "\n",
        "print(df1)\n",
        "print('------------')\n",
        "\n",
        "df1.insert(1, \"order\", np.arange(1, 5)) # DataFrame.insert(loc, column, value, allow_duplicates=_NoDefault.no_default)\n",
        "print(df1)"
      ],
      "metadata": {
        "id": "r7pE7YaLxt6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Delete columns\n",
        "del df1[\"minutes\"]\n",
        "df1.pop(\"seconds\")\n",
        "\n",
        "df1"
      ],
      "metadata": {
        "id": "Vk3_TKQWx2hF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transposing\n",
        "print(df1.transpose())\n",
        "print('------------')\n",
        "print(df1[:2].transpose()) # Restrict observations to 2nd row and then transpose"
      ],
      "metadata": {
        "id": "2ENSLK0i8wQV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.2.2. Merging and Joining**"
      ],
      "metadata": {
        "id": "vTKPSJPJH6CT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`pandas` provides various facilities for easily combining together `Series` or `DataFrame` with various kinds of set logic for the indexes and relational algebra functionality in the case of join / merge-type operations. Also, it has full-featured, high performance in-memory join operations idiomatically very similar to relational databases like SQL.\n",
        "\n",
        "This library offers a single function, `merge()`, as the entry point for all standard database join operations between `DataFrame` or named `Series` objects:\n",
        "\n",
        "```python\n",
        "pd.merge(\n",
        "    left,\n",
        "    right,\n",
        "    how=\"inner\",\n",
        "    on=None,\n",
        "    left_on=None,\n",
        "    right_on=None,\n",
        "    left_index=False,\n",
        "    right_index=False,\n",
        "    sort=True,\n",
        "    suffixes=(\"_x\", \"_y\"),\n",
        "    copy=True,\n",
        "    indicator=False,\n",
        "    validate=None,\n",
        ")\n",
        "```\n",
        "\n",
        "* `left`: A DataFrame or named Series object.\n",
        "* `right`: Another DataFrame or named Series object.\n",
        "* `on`: Column or index level names to join on. Must be found in both the left and right DataFrame and/or Series objects. If not passed and `left_index` and `right_index` are `False`, the intersection of the columns in the DataFrames and/or Series will be inferred to be the join keys.\n",
        "* `left_on`: Columns or index levels from the left DataFrame or Series to use as keys. Can either be column names, index level names, or arrays with length equal to the length of the DataFrame or Series.\n",
        "* `right_on`: Columns or index levels from the right DataFrame or Series to use as keys. Can either be column names, index level names, or arrays with length equal to the length of the DataFrame or Series.\n",
        "* `left_index`: If `True`, use the index (row labels) from the left DataFrame or Series as its join key(s). In the case of a DataFrame or Series with a MultiIndex (hierarchical), the number of levels must match the number of join keys from the right DataFrame or Series.\n",
        "* `right_index`: Same usage as `left_index` for the right DataFrame or Series\n",
        "* `how`: One of `'left'`, `'right'`, `'outer'`, `'inner'`, `'cross'`. Defaults to inner.\n",
        "* `sort`: Sort the result DataFrame by the join keys in lexicographical order. Defaults to `True`, setting to `False` will improve performance substantially in many cases.\n",
        "* `suffixes`: A tuple of string suffixes to apply to overlapping columns. Defaults to `('_x', '_y')`.\n",
        "* `copy`: Always copy data (default `True`) from the passed DataFrame or named Series objects, even when reindexing is not necessary. Cannot be avoided in many cases but may improve performance / memory usage. The cases where copying can be avoided are somewhat pathological but this option is provided nonetheless.\n",
        "* `indicator`: Add a column to the output DataFrame called `_merge` with information on the source of each row. `_merge` is Categorical-type and takes on a value of `left_only` for observations whose merge key only appears in `'left'` DataFrame or Series, `right_only` for observations whose merge key only appears in `'right'` DataFrame or Series, and `both` if the observationâ€™s merge key is found in both.\n",
        "* `validate`: string, default `None`. If specified, checks if merge is of specified type.\n",
        "\n",
        "For your information, please refer to the following references:\n",
        "* [**Pandas Merge, Join, Concatenate, and Compare**](https://pandas.pydata.org/docs/user_guide/merging.html)\n",
        "* [**Pandas Codebook**](https://pandas.pydata.org/docs/user_guide/cookbook.html)\n",
        "* [**Pandas Comparison with SQL**](https://pandas.pydata.org/docs/getting_started/comparison/comparison_with_sql.html)"
      ],
      "metadata": {
        "id": "B30OmXpEDyNA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image](https://media.geeksforgeeks.org/wp-content/uploads/joinimages.png)"
      ],
      "metadata": {
        "id": "fpHxtCAxj__F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Left join with a single key\n",
        "left = pd.DataFrame(\n",
        "    {\n",
        "        \"key\": [\"K0\", \"K1\", \"K2\", \"K3\"],\n",
        "        \"A\": [\"A0\", \"A1\", \"A2\", \"A3\"],\n",
        "        \"B\": [\"B0\", \"B1\", \"B2\", \"B3\"],\n",
        "    }\n",
        ")\n",
        "\n",
        "right = pd.DataFrame(\n",
        "    {\n",
        "        \"key\": [\"K0\", \"K1\", \"K2\", \"K3\"],\n",
        "        \"C\": [\"C0\", \"C1\", \"C2\", \"C3\"],\n",
        "        \"D\": [\"D0\", \"D1\", \"D2\", \"D3\"],\n",
        "    }\n",
        ")\n",
        "\n",
        "result = pd.merge(left, right, how=\"left\", on=\"key\")\n",
        "result"
      ],
      "metadata": {
        "id": "fyY_UjIhH608"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Left join with two keys\n",
        "left = pd.DataFrame(\n",
        "    {\n",
        "        \"key1\": [\"K0\", \"K0\", \"K1\", \"K1\"],\n",
        "        \"key2\": [\"L0\", \"L1\", \"L0\", \"L1\"],\n",
        "        \"A\": [\"A0\", \"A1\", \"A2\", \"A3\"],\n",
        "        \"B\": [\"B0\", \"B1\", \"B2\", \"B3\"],\n",
        "    }\n",
        ")\n",
        "\n",
        "right = pd.DataFrame(\n",
        "    {\n",
        "        \"key1\": [\"K0\", \"K1\", \"K1\", \"K2\"],\n",
        "        \"key2\": [\"L0\", \"L0\", \"L0\", \"L0\"],\n",
        "        \"C\": [\"C0\", \"C1\", \"C2\", \"C3\"],\n",
        "        \"D\": [\"D0\", \"D1\", \"D2\", \"D3\"],\n",
        "    }\n",
        ")\n",
        "\n",
        "result = pd.merge(left, right, how=\"left\", on=[\"key1\", \"key2\"])\n",
        "result"
      ],
      "metadata": {
        "id": "LrilrMucOfMd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Right join with two keys (using 'left' and 'right' defined above)\n",
        "result = pd.merge(left, right, how=\"right\", on=[\"key1\", \"key2\"]) # Set 'right' join\n",
        "result"
      ],
      "metadata": {
        "id": "qJxSYiKsPcIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Outer join with two keys (using 'left' and 'right' defined above)\n",
        "result = pd.merge(left, right, how=\"outer\", on=[\"key1\", \"key2\"]) # Set 'outer' join\n",
        "result"
      ],
      "metadata": {
        "id": "4XH1_PelO5zW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inner join with two keys (using 'left' and 'right' defined above)\n",
        "result = pd.merge(left, right, how=\"inner\", on=[\"key1\", \"key2\"]) # Set 'inner' join\n",
        "result"
      ],
      "metadata": {
        "id": "szlHvkHLPoL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Outer join with two keys + Indicator (using 'left' and 'right' defined above)\n",
        "result = pd.merge(left, right, how=\"outer\", on=[\"key1\", \"key2\"], indicator=\"matched\") # Set 'outer' join + Indicator\n",
        "result"
      ],
      "metadata": {
        "id": "Ds64LjxvQRBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`DataFrame.join()` is a convenient method for combining the columns of two potentially differently-indexed DataFrames into a single result DataFrame.\n",
        "\n",
        "```python\n",
        "DataFrame.join(\n",
        "  other,\n",
        "  on=None,\n",
        "  how='left',\n",
        "  lsuffix='',\n",
        "  rsuffix='',\n",
        "  sort=False,\n",
        "  validate=None\n",
        "  )\n",
        "```\n",
        "\n",
        "Please run the following code to see how simple it is to join DataFrames with an index."
      ],
      "metadata": {
        "id": "K2Vf928PRI2k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define two dataframes sharing an index\n",
        "left = pd.DataFrame(\n",
        "    {\"A\": [\"A0\", \"A1\", \"A2\"], \"B\": [\"B0\", \"B1\", \"B2\"]}, index=[\"K0\", \"K1\", \"K2\"]\n",
        ")\n",
        "\n",
        "\n",
        "right = pd.DataFrame(\n",
        "    {\"C\": [\"C0\", \"C2\", \"C3\"], \"D\": [\"D0\", \"D2\", \"D3\"]}, index=[\"K0\", \"K2\", \"K3\"]\n",
        ")"
      ],
      "metadata": {
        "id": "diNr47EDRpyl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The two codes yield the same outcome\n",
        "print(left.join(right))\n",
        "print('------------')\n",
        "print(pd.merge(left, right, how=\"left\", left_index=True, right_index=True))"
      ],
      "metadata": {
        "id": "ALzh8IgjR0e3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The two codes yield the same outcome\n",
        "print(left.join(right, how=\"outer\"))\n",
        "print('------------')\n",
        "print(pd.merge(left, right, how=\"outer\", left_index=True, right_index=True))"
      ],
      "metadata": {
        "id": "r-PxavrBSsPL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GroupBy**\n",
        "\n",
        "A `groupby()` operation involves some combination of splitting the object, applying a function, and combining the results. This can be used to group large amounts of data and compute operations on these groups.\n",
        "\n",
        "```python\n",
        "DataFrame.groupby(\n",
        "  by=None,\n",
        "  axis=0,\n",
        "  level=None,\n",
        "  as_index=True,\n",
        "  sort=True,\n",
        "  group_keys=_NoDefault.no_default,\n",
        "  squeeze=_NoDefault.no_default,\n",
        "  observed=False,\n",
        "  dropna=True\n",
        "  )\n",
        "```\n",
        "\n",
        "Please refer to [**pandas.DataFrame.groupby**](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html) for details of syntax."
      ],
      "metadata": {
        "id": "p9SqNOTEGfJ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the code to create a dataframe (24 rows x 6 columns)\n",
        "import datetime\n",
        "\n",
        "df3 = pd.DataFrame(\n",
        "    {\n",
        "        \"Z\": [1,1,1,2,2,2,3,3,3,4,4,4,5,5,5,6,6,6,7,7,7,8,8,8],\n",
        "        \"A\": [\"one\", \"one\", \"two\", \"three\"] * 6,\n",
        "        \"B\": [\"a\", \"b\", \"c\"] * 8,\n",
        "        \"C\": [\"foo\", \"foo\", \"foo\", \"bar\", \"bar\", \"bar\"] * 4,\n",
        "        \"D\": np.random.randn(24),\n",
        "        \"E\": np.random.randn(24),\n",
        "        \"F\": [datetime.datetime(2024, i, 1) for i in range(1, 13)]\n",
        "        + [datetime.datetime(2024, i, 15) for i in range(1, 13)],\n",
        "    }\n",
        ")\n",
        "print(df3)"
      ],
      "metadata": {
        "id": "Vj1XahQPmxdS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Briefly explore 'df3'\n",
        "df3.head(5)"
      ],
      "metadata": {
        "id": "IC4T4XnoGot7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply 'groupby' (worked in 2024 but not yielding errors now)\n",
        "print(df3.groupby([\"A\"]).mean()) # Mean by A\n",
        "print('------------')\n",
        "print(df3.groupby([\"A\", \"B\"]).mean()) # Mean by (A x B) combination\n",
        "print('------------')\n",
        "print(df3.groupby([\"A\", \"B\"]).sum()) # Sum by (A x B) combination"
      ],
      "metadata": {
        "id": "xSneeXnmGyaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply 'groupby' (revised in Jan 24, 2025)\n",
        "print(df3.groupby([\"A\"]).mean([\"D\", \"E\", \"F\"])) # Mean by A\n",
        "print('------------')\n",
        "print(df3.groupby([\"A\", \"B\"]).mean([\"D\", \"E\", \"F\"])) # Mean by (A x B) combination\n",
        "print('------------')\n",
        "print(df3.groupby([\"A\", \"B\"]).sum([\"D\", \"E\", \"F\"])) # Sum by (A x B) combination"
      ],
      "metadata": {
        "id": "U4DADlr6nDol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Part 3. Handling Incomplete Data**"
      ],
      "metadata": {
        "id": "aycN11iVwQJZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try handling incomplete data! Datasets often have various problems, such as missing data, inconsistent format, and incorrect values. The following practices will help you grasp potential problems that you might encounter in the future."
      ],
      "metadata": {
        "id": "k1zWGycyTqrp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 3. Variable transformation and missing data**\n",
        "\n",
        "**Skewness** is a measure of the asymmetry of the probability distribution of a real-valued random variable about its mean. The skewness value can be positive, zero, negative, or undefined. For a unimodal distribution, negative skew commonly indicates that the tail is on the left side of the distribution, and positive skew indicates that the tail is on the right.\n",
        "\n",
        "![Image](https://upload.wikimedia.org/wikipedia/commons/c/cc/Relationship_between_mean_and_median_under_different_skewness.png)\n",
        "\n",
        "When a distribution is highly skewed, transforming a variable often helps in estimating a statistical model. Among various transformations, you will try three functions are widely used in social science.\n",
        "\n",
        "Please revise the incomplete code to create the following variables:\n",
        "\n",
        "1. $x1 = log(x)$\n",
        "1. $x2 = log(x+1)$\n",
        "1. $x3 = arcsinh(x)$\n",
        "\n",
        "where the inverse hyperbolic sine function is defined as $arcsinh(x) \\equiv ln(x + \\sqrt{x^2+1})$."
      ],
      "metadata": {
        "id": "JfeGy9o_qKal"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this code to generate a dataframe\n",
        "df = pd.DataFrame(\n",
        "    {\n",
        "        \"x\": [-1.000231, -0.449368, -0.001214, 0, 0.004332, 0.019371, 0.021872, 0.248100, 0.649678, 1.248371],\n",
        "    }\n",
        ")\n",
        "df"
      ],
      "metadata": {
        "id": "V4DWYuXxqJXV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (1) Create `x1` by transforming `x` to `log(x)`\n",
        "df['x1'] = np.log(df['x'])"
      ],
      "metadata": {
        "id": "gzH3Cgd_hzWo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (2) Create `x2` by transforming `x` to `log(x+1)`\n",
        "df['x2'] = np.log(df['x'] + 1)"
      ],
      "metadata": {
        "id": "XVmwlF7yiJo2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (3) Create `x3` by transforming `x` to `asinh(x)`\n",
        "df['x3'] = np.log(df['x'] + (df['x'] ** 2 + 1) ** 0.5) # Try to model the `asinh(x)` function instead of directly using `numpy.arcsinh()`"
      ],
      "metadata": {
        "id": "Fk_XXBJkkAKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the code to view the revised columns\n",
        "df"
      ],
      "metadata": {
        "id": "Tjtw8Xpij_Zm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Takeaway:** The log transformation may lead to missing values due to its functional restriction. Using alternative functions can address this issue in a different way. For more information about its background, application, and interpretation, please refer to the following paper:\n",
        "* Jaeung Sim, Jea Gon Park, Daegon Cho, Michael D. Smith, and Jaemin Jung (2022) \"[Bestseller lists and product discovery in the subscription-based market: Evidence from music streaming](https://doi.org/10.1016/j.jebo.2021.12.030),\" *Journal of Economic Behavior & Organization* 194, pp. 550-567."
      ],
      "metadata": {
        "id": "7BO_rZlUkVcF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Part 4: Data Processing with External Data**"
      ],
      "metadata": {
        "id": "USgA_k7J5Sf7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Setup**"
      ],
      "metadata": {
        "id": "c0z-4H6H-Z1h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set your Google Drive directory\n",
        "import os\n",
        "os.getcwd()\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "os.chdir('/content/drive/My Drive/Colab Notebooks/OPIM 5512 (Spring 2025)') # You may need to change this directory"
      ],
      "metadata": {
        "id": "82o9EySW-PP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this code to import the NumPy and Pandas modules\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "gvrYK9ar-cmN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exploring the Data**"
      ],
      "metadata": {
        "id": "2_adXOz1-fXP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read external data\n",
        "df = pd.read_csv('dataset_notebook_02.csv')\n",
        "df # Print the whole DataFrame"
      ],
      "metadata": {
        "id": "pO_tpV7L-fn9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Head\n",
        "df.head()"
      ],
      "metadata": {
        "id": "2-vIFJiUXnQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(10)"
      ],
      "metadata": {
        "id": "TspVWZN9XoRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tail\n",
        "df.tail()"
      ],
      "metadata": {
        "id": "mVdnOR6yXrIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Information about the data\n",
        "df.info()"
      ],
      "metadata": {
        "id": "GeT1JyeqXsKO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 4. Cleaning an external dataset**\n",
        "\n",
        "Construct a dataframe from an external csv file (`dataset_notebook_02.csv`). This data has several flaws that you need to correct. Let's revise the following incomplete code together to satisfy the followings:\n",
        "\n",
        "1. Drop rows with empty cells.\n",
        "\n",
        "   Tip: `pandas` has a `DataFrame.dropna()` method\n",
        "\n",
        "1. Correct the format of `Date` as `datetime`.\n",
        "\n",
        "   Tip: `pandas` has a `pd.to_datetime()` method\n",
        "\n",
        "1. Replace duration value `450` in row 7 with `45`.\n",
        "\n",
        "1. Remove duplicates."
      ],
      "metadata": {
        "id": "BvPz9PILX_41"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read external data\n",
        "df = pd.read_csv('dataset_notebook_02.csv')\n",
        "df # Print the whole DataFrame"
      ],
      "metadata": {
        "id": "pN42GjWybWjN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data set contains some empty cells (\"Date\" in row 22, and \"Calories\" in row 18 and 28), wrong format (\"Date\" in row 26), wrong data (\"Duration\" in row 7), and duplicates (row 11 and 12)."
      ],
      "metadata": {
        "id": "5eE3MGgNZBVi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# (1) Drop rows with empty cells\n",
        "df.dropna(inplace = True)\n",
        "df"
      ],
      "metadata": {
        "id": "04e8lS80Ycbb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (2) Correct the format of 'Date' as 'datetime'\n",
        "df['Date'] = pd.to_datetime(df['Date']) # This yields errors (a year ago, it worked...)"
      ],
      "metadata": {
        "id": "fkOtzWKIcp-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Date_backup'] = df['Date'] # We need a backup for this column"
      ],
      "metadata": {
        "id": "CHf69yvbe-Tr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Date'] = pd.to_datetime(df['Date'], errors='coerce') # This will generate NaT values\n",
        "df"
      ],
      "metadata": {
        "id": "pWURPy3iYeLp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.loc[df_backup['Date'].isna(), 'Date'] = df['Date_backup'] # Retrieve the original value for NaT values\n",
        "df"
      ],
      "metadata": {
        "id": "cL6C8NVBfIfq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (3) Correct wrong data\n",
        "df.loc[7, 'Duration'] = 45 # Replace duration value '450' in row 7 with '45' (Tip: DataFrame.loc[row index, 'column name'])\n",
        "df"
      ],
      "metadata": {
        "id": "wSZz1c74YfjK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (4) Remove duplicates\n",
        "df.drop_duplicates(inplace = True) # Tip: DataFrame.drop_duplicates(inplace = True)\n",
        "df"
      ],
      "metadata": {
        "id": "v_vDSdo4Yigw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}